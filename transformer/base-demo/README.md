# Transformer ä¸­è‹±æ–‡ç¿»è¯‘ Demo

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ Transformer æ¨¡å‹å®ç°ï¼Œç”¨äºä¸­è‹±æ–‡ç¿»è¯‘ä»»åŠ¡ã€‚æœ¬é¡¹ç›®ä¸“ä¸ºæ•™å­¦è®¾è®¡ï¼Œä»£ç ç»“æ„æ¸…æ™°ï¼Œæ³¨é‡Šè¯¦å°½ï¼Œå±•ç¤ºäº† Transformer æ¶æ„çš„æ ¸å¿ƒç‰¹å¾ã€‚

## ğŸ¯ é¡¹ç›®ç‰¹ç‚¹

### Transformer æ ¸å¿ƒç‰¹å¾å±•ç¤º
- âœ… **å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶** (Multi-Head Attention)
- âœ… **ä½ç½®ç¼–ç ** (Positional Encoding)
- âœ… **ç¼–ç å™¨-è§£ç å™¨æ¶æ„** (Encoder-Decoder)
- âœ… **æ®‹å·®è¿æ¥** (Residual Connections)
- âœ… **å±‚å½’ä¸€åŒ–** (Layer Normalization)
- âœ… **å‰é¦ˆç¥ç»ç½‘ç»œ** (Feed-Forward Networks)
- âœ… **æ©ç æœºåˆ¶** (Masking)
- âœ… **æ ‡ç­¾å¹³æ»‘** (Label Smoothing)
- âœ… **å­¦ä¹ ç‡è°ƒåº¦** (Learning Rate Scheduling)

### æ•™å­¦å‹å¥½è®¾è®¡
- ğŸ“š è¯¦å°½çš„ä»£ç æ³¨é‡Šï¼Œè§£é‡Šæ¯ä¸ªç»„ä»¶çš„ä½œç”¨
- ğŸ—ï¸ æ¨¡å—åŒ–è®¾è®¡ï¼Œä¾¿äºç†è§£å’Œä¿®æ”¹
- ğŸ“Š è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–
- ğŸ”§ å®Œæ•´çš„é…ç½®ç®¡ç†
- ğŸš€ ä¸€é”®å¯åŠ¨è®­ç»ƒ

## ğŸ“ é¡¹ç›®ç»“æ„

```
transformer/base-demo/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â”œâ”€â”€ computer_en_26k.jsonl    # è‹±æ–‡æ•°æ®é›†
â”‚       â””â”€â”€ computer_zh_26k.jsonl    # ä¸­æ–‡æ•°æ®é›†
â”œâ”€â”€ models/                          # æ¨¡å‹ä¿å­˜ç›®å½•
â”œâ”€â”€ config.py                        # é…ç½®æ–‡ä»¶
â”œâ”€â”€ data_processor.py               # æ•°æ®å¤„ç†æ¨¡å—
â”œâ”€â”€ transformer_model.py            # Transformer æ¨¡å‹å®ç°
â”œâ”€â”€ trainer.py                      # è®­ç»ƒå™¨æ¨¡å—
â”œâ”€â”€ main.py                         # ä¸»è®­ç»ƒè„šæœ¬
â”œâ”€â”€ requirements.txt                # ä¾èµ–åŒ…åˆ—è¡¨
â””â”€â”€ README.md                       # é¡¹ç›®è¯´æ˜
```

## ğŸ› ï¸ ç¯å¢ƒè¦æ±‚

- Python 3.8+
- PyTorch 2.0+
- CUDA (å¯é€‰ï¼Œç”¨äº GPU åŠ é€Ÿ)

## ğŸ“¦ å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æ•°æ®å‡†å¤‡
ç¡®ä¿æ•°æ®æ–‡ä»¶ä½äºæ­£ç¡®ä½ç½®ï¼š
- `data/raw/computer_en_26k.jsonl`
- `data/raw/computer_zh_26k.jsonl`

### 2. å¼€å§‹è®­ç»ƒ
```bash
python main.py
```

### 3. è‡ªå®šä¹‰è®­ç»ƒå‚æ•°
```bash
# æŒ‡å®šè®­ç»ƒè½®æ•°
python main.py --epochs 20

# æŒ‡å®šæ‰¹æ¬¡å¤§å°
python main.py --batch-size 64

# æŒ‡å®šå­¦ä¹ ç‡
python main.py --learning-rate 0.0001

# ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
python main.py --resume best_model_epoch_10.pth

# ä»…æµ‹è¯•æ¨¡å¼
python main.py --test-only --resume best_model_epoch_10.pth
```

## ğŸ—ï¸ æ¶æ„è¯¦è§£

### 1. Transformer æ•´ä½“æ¶æ„

```
è¾“å…¥åºåˆ— â†’ ç¼–ç å™¨ â†’ ç¼–ç å™¨è¾“å‡º
                      â†“
ç›®æ ‡åºåˆ— â†’ è§£ç å™¨ â† ç¼–ç å™¨è¾“å‡º â†’ è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ
```

### 2. ç¼–ç å™¨ (Encoder)

æ¯ä¸ªç¼–ç å™¨å±‚åŒ…å«ï¼š
1. **å¤šå¤´è‡ªæ³¨æ„åŠ›** - æ•è·åºåˆ—å†…éƒ¨ä¾èµ–å…³ç³»
2. **æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–** - ç¨³å®šè®­ç»ƒè¿‡ç¨‹
3. **å‰é¦ˆç¥ç»ç½‘ç»œ** - éçº¿æ€§å˜æ¢
4. **æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–** - å†æ¬¡ç¨³å®šè®­ç»ƒ

```python
class EncoderLayer(nn.Module):
    def forward(self, x, mask=None):
        # å¤šå¤´è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        attn_output = self.self_attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x
```

### 3. è§£ç å™¨ (Decoder)

æ¯ä¸ªè§£ç å™¨å±‚åŒ…å«ï¼š
1. **æ©ç è‡ªæ³¨æ„åŠ›** - é˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯
2. **ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›** - å…³æ³¨æºåºåˆ—ä¿¡æ¯
3. **å‰é¦ˆç¥ç»ç½‘ç»œ** - éçº¿æ€§å˜æ¢

```python
class DecoderLayer(nn.Module):
    def forward(self, x, encoder_output, self_mask=None, cross_mask=None):
        # æ©ç è‡ªæ³¨æ„åŠ›
        self_attn_output = self.self_attention(x, x, x, self_mask)
        x = self.norm1(x + self.dropout(self_attn_output))
        
        # ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›
        cross_attn_output = self.cross_attention(x, encoder_output, encoder_output, cross_mask)
        x = self.norm2(x + self.dropout(cross_attn_output))
        
        # å‰é¦ˆç½‘ç»œ
        ff_output = self.feed_forward(x)
        x = self.norm3(x + self.dropout(ff_output))
        
        return x
```

### 4. å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶

æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒå…¬å¼ï¼š
```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k)V
```

å¤šå¤´æ³¨æ„åŠ›å°†è¾“å…¥æŠ•å½±åˆ°å¤šä¸ªå­ç©ºé—´ï¼š
```python
def scaled_dot_product_attention(self, Q, K, V, mask=None):
    # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
    
    # åº”ç”¨æ©ç 
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    
    # Softmax å½’ä¸€åŒ–
    attention_weights = F.softmax(scores, dim=-1)
    
    # åŠ æƒæ±‚å’Œ
    output = torch.matmul(attention_weights, V)
    return output, attention_weights
```

### 5. ä½ç½®ç¼–ç 

ç”±äº Transformer æ²¡æœ‰å¾ªç¯ç»“æ„ï¼Œéœ€è¦ä½ç½®ç¼–ç æ¥æä¾›ä½ç½®ä¿¡æ¯ï¼š

```python
# å¶æ•°ä½ç½®ä½¿ç”¨ sinï¼Œå¥‡æ•°ä½ç½®ä½¿ç”¨ cos
pe[:, 0::2] = torch.sin(position * div_term)
pe[:, 1::2] = torch.cos(position * div_term)
```

## ğŸ“Š è®­ç»ƒç›‘æ§

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šæ˜¾ç¤ºï¼š
- è®­ç»ƒæŸå¤±å’ŒéªŒè¯æŸå¤±
- å›°æƒ‘åº¦ (Perplexity)
- å­¦ä¹ ç‡å˜åŒ–
- è®­ç»ƒæ—¶é—´

è®­ç»ƒå®Œæˆåä¼šç”Ÿæˆè®­ç»ƒæ›²çº¿å›¾ï¼š`training_curves.png`

## ğŸ›ï¸ é…ç½®è¯´æ˜

ä¸»è¦è¶…å‚æ•°åœ¨ `config.py` ä¸­é…ç½®ï¼š

```python
class Config:
    # æ¨¡å‹è¶…å‚æ•°
    D_MODEL = 512          # æ¨¡å‹ç»´åº¦
    N_HEADS = 8            # æ³¨æ„åŠ›å¤´æ•°
    N_LAYERS = 6           # ç¼–ç å™¨/è§£ç å™¨å±‚æ•°
    D_FF = 2048            # å‰é¦ˆç½‘ç»œç»´åº¦
    DROPOUT = 0.1          # Dropout æ¦‚ç‡
    MAX_SEQ_LEN = 256      # æœ€å¤§åºåˆ—é•¿åº¦
    
    # è®­ç»ƒé…ç½®
    BATCH_SIZE = 32        # æ‰¹æ¬¡å¤§å°
    LEARNING_RATE = 1e-4   # å­¦ä¹ ç‡
    NUM_EPOCHS = 10        # è®­ç»ƒè½®æ•°
    WARMUP_STEPS = 4000    # å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°
    LABEL_SMOOTHING = 0.1  # æ ‡ç­¾å¹³æ»‘
```

## ğŸ”§ é«˜çº§ç‰¹æ€§

### 1. æ ‡ç­¾å¹³æ»‘ (Label Smoothing)
å‡å°‘è¿‡æ‹Ÿåˆï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ï¼š
```python
class LabelSmoothingLoss(nn.Module):
    def __init__(self, vocab_size, smoothing=0.1):
        self.smoothing = smoothing
        self.confidence = 1.0 - smoothing
```

### 2. Noam å­¦ä¹ ç‡è°ƒåº¦
éµå¾ªåŸå§‹ Transformer è®ºæ–‡çš„å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ï¼š
```python
lr = (d_model ** -0.5) * min(step_num ** -0.5, step_num * warmup_steps ** -1.5)
```

### 3. æ¢¯åº¦è£å‰ª
é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼š
```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **GPU åŠ é€Ÿ**ï¼šç¡®ä¿å®‰è£…äº† CUDA ç‰ˆæœ¬çš„ PyTorch
2. **æ‰¹æ¬¡å¤§å°**ï¼šæ ¹æ® GPU å†…å­˜è°ƒæ•´ `BATCH_SIZE`
3. **åºåˆ—é•¿åº¦**ï¼šæ ¹æ®æ•°æ®ç‰¹ç‚¹è°ƒæ•´ `MAX_SEQ_LEN`
4. **æ¨¡å‹å¤§å°**ï¼šå¯ä»¥è°ƒæ•´ `D_MODEL`ã€`N_HEADS`ã€`N_LAYERS` ç­‰å‚æ•°

## ğŸ› å¸¸è§é—®é¢˜

### Q: è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç° CUDA å†…å­˜ä¸è¶³
A: å‡å° `BATCH_SIZE` æˆ– `MAX_SEQ_LEN`

### Q: è®­ç»ƒæŸå¤±ä¸ä¸‹é™
A: æ£€æŸ¥å­¦ä¹ ç‡è®¾ç½®ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´ `LEARNING_RATE` æˆ– `WARMUP_STEPS`

### Q: ç¿»è¯‘è´¨é‡ä¸å¥½
A: å¢åŠ è®­ç»ƒè½®æ•°ï¼Œæˆ–è€…ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ï¼ˆå¢åŠ  `D_MODEL`ã€`N_LAYERS`ï¼‰

### Q: æ•°æ®åŠ è½½å¤±è´¥
A: ç¡®ä¿æ•°æ®æ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼Œæ–‡ä»¶æ ¼å¼ä¸º JSONL

## ğŸ“š å­¦ä¹ èµ„æº

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformer åŸå§‹è®ºæ–‡
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - å¯è§†åŒ–è§£é‡Š
- [PyTorch Transformer Tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html) - å®˜æ–¹æ•™ç¨‹

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Request æ¥æ”¹è¿›è¿™ä¸ªæ•™å­¦é¡¹ç›®ï¼

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ã€‚

---

**æ³¨æ„**ï¼šè¿™æ˜¯ä¸€ä¸ªæ•™å­¦ç”¨çš„ç®€åŒ–å®ç°ï¼Œä¸»è¦ç›®çš„æ˜¯å±•ç¤º Transformer çš„æ ¸å¿ƒæ¦‚å¿µã€‚åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œå»ºè®®ä½¿ç”¨æ›´æˆç†Ÿçš„æ¡†æ¶å¦‚ Hugging Face Transformersã€‚