"""
ç¬¬4èŠ‚ï¼šå¤šæ¨¡å‹æ¥å…¥ä¸åˆ‡æ¢
=============================================

å­¦ä¹ ç›®æ ‡ï¼š
- äº†è§£ä¸»æµ LLM æœåŠ¡å•†å’Œæ¨¡å‹ç‰¹ç‚¹
- å­¦ä¼šæ¥å…¥ä¸åŒçš„æ¨¡å‹æœåŠ¡
- æŒæ¡æ¨¡å‹åˆ‡æ¢å’Œé…ç½®ç®¡ç†
- ç†è§£æ¨¡å‹é€‰æ‹©ç­–ç•¥å’Œæˆæœ¬ä¼˜åŒ–

å‰ç½®çŸ¥è¯†ï¼š
- å®Œæˆç¬¬1-3èŠ‚åŸºç¡€å†…å®¹

é‡ç‚¹æ¦‚å¿µï¼š
- ä¸åŒæ¨¡å‹æœ‰ä¸åŒçš„ç‰¹ç‚¹å’Œä»·æ ¼
- ç»Ÿä¸€æ¥å£è®©åˆ‡æ¢å˜å¾—ç®€å•
- æ ¹æ®ä»»åŠ¡é€‰æ‹©åˆé€‚çš„æ¨¡å‹
"""

import os
from typing import Dict, List, Optional
from dataclasses import dataclass


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®ç±»"""
    name: str
    provider: str
    api_base: str
    price_input: float  # æ¯1K tokensä»·æ ¼(å…ƒ)
    price_output: float
    max_tokens: int
    strengths: List[str]


def explain_model_landscape():
    """
    ä»‹ç» LLM æ¨¡å‹æ ¼å±€
    """
    print("\n" + "="*60)
    print("ğŸŒ LLM æ¨¡å‹ç”Ÿæ€å…¨æ™¯")
    print("="*60)
    
    print("""
ğŸ¢ ä¸»è¦æœåŠ¡å•†åˆ†ç±»ï¼š

å›½é™…å·¨å¤´ï¼š
  ğŸ‡ºğŸ‡¸ OpenAIï¼šGPT-4, GPT-3.5ï¼ˆåŠŸèƒ½æœ€å¼ºï¼Œä»·æ ¼è¾ƒé«˜ï¼‰
  ğŸ‡ºğŸ‡¸ Anthropicï¼šClaudeï¼ˆå®‰å…¨æ€§å¼ºï¼Œæ¨ç†èƒ½åŠ›å¥½ï¼‰
  ğŸ‡ºğŸ‡¸ Googleï¼šGeminiï¼ˆå¤šæ¨¡æ€èƒ½åŠ›å¼ºï¼‰

å›½äº§åŠ›é‡ï¼š
  ğŸ‡¨ğŸ‡³ DeepSeekï¼šä¾¿å®œå®ç”¨ï¼Œæ€§ä»·æ¯”ä¹‹ç‹
  ğŸ‡¨ğŸ‡³ é˜¿é‡Œï¼šé€šä¹‰åƒé—®ï¼Œä¸­æ–‡ç†è§£å¥½
  ğŸ‡¨ğŸ‡³ ç™¾åº¦ï¼šæ–‡å¿ƒä¸€è¨€ï¼Œå›½å†…ç”Ÿæ€å®Œå–„
  ğŸ‡¨ğŸ‡³ å­—èŠ‚ï¼šè±†åŒ…ï¼ŒçŸ­æ–‡æœ¬å¤„ç†å¿«

å¼€æºæ¨¡å‹ï¼š
  ğŸ”“ Llamaï¼šMetaå¼€æºï¼Œå¯ç§æœ‰éƒ¨ç½²
  ğŸ”“ Mistralï¼šæ¬§æ´²å¼€æºï¼Œæ•ˆç‡å¾ˆé«˜
  ğŸ”“ Qwenï¼šé˜¿é‡Œå¼€æºï¼Œä¸­æ–‡å‹å¥½
    """)
    
    # æ¨¡å‹é…ç½®å¯¹æ¯”è¡¨
    models = [
        ModelConfig("gpt-4", "OpenAI", "api.openai.com", 0.21, 0.42, 8192, 
                   ["æ¨ç†èƒ½åŠ›å¼º", "é€šç”¨æ€§å¥½", "æ–‡æ¡£ä¸°å¯Œ"]),
        ModelConfig("deepseek-chat", "DeepSeek", "api.deepseek.com", 0.0014, 0.0028, 4096,
                   ["æ€§ä»·æ¯”é«˜", "å“åº”å¿«", "ä¸­æ–‡å‹å¥½"]),
        ModelConfig("claude-3", "Anthropic", "api.anthropic.com", 0.21, 0.42, 200000,
                   ["å®‰å…¨æ€§å¼º", "é•¿æ–‡æœ¬", "æ¨ç†å‡†ç¡®"]),
        ModelConfig("qwen-max", "é˜¿é‡Œäº‘", "dashscope.aliyuncs.com", 0.12, 0.12, 8192,
                   ["ä¸­æ–‡ä¼˜ç§€", "å›½å†…è®¿é—®", "ç”Ÿæ€å®Œå–„"])
    ]
    
    print("\nğŸ“Š æ¨¡å‹å¯¹æ¯”è¡¨ï¼š")
    print("-" * 80)
    print(f"{'æ¨¡å‹åç§°':<15} {'æœåŠ¡å•†':<10} {'è¾“å…¥ä»·æ ¼':<10} {'è¾“å‡ºä»·æ ¼':<10} {'ä¸»è¦ä¼˜åŠ¿'}")
    print("-" * 80)
    
    for model in models:
        strengths = ", ".join(model.strengths[:2])
        print(f"{model.name:<15} {model.provider:<10} {model.price_input:<10.4f} {model.price_output:<10.4f} {strengths}")


def setup_multi_models():
    """
    é…ç½®å¤šä¸ªæ¨¡å‹
    """
    print("\n" + "="*60)
    print("ğŸ”§ é…ç½®å¤šä¸ªæ¨¡å‹æœåŠ¡")
    print("="*60)
    
    try:
        from langchain_openai import ChatOpenAI
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.output_parsers import StrOutputParser
        
        print("ğŸ¯ æ­¥éª¤1ï¼šé…ç½® DeepSeek æ¨¡å‹")
        print("-" * 30)
        
        deepseek_key = os.getenv("DEEPSEEK_API_KEY")
        if deepseek_key:
            deepseek_model = ChatOpenAI(
                model="deepseek-chat",
                openai_api_key=deepseek_key,
                openai_api_base="https://api.deepseek.com",
                temperature=0.7,
                max_tokens=1000
            )
            print("âœ… DeepSeek é…ç½®æˆåŠŸ")
        else:
            print("âŒ æœªæ‰¾åˆ° DEEPSEEK_API_KEY")
            deepseek_model = None
        
        print("\nğŸ¯ æ­¥éª¤2ï¼šé…ç½® SiliconFlow æ¨¡å‹")
        print("-" * 30)
        
        siliconflow_key = os.getenv("SILICONFLOW_API_KEY")
        if siliconflow_key:
            siliconflow_model = ChatOpenAI(
                model="Qwen/Qwen2.5-7B-Instruct",
                openai_api_key=siliconflow_key,
                openai_api_base="https://api.siliconflow.cn/v1",
                temperature=0.7,
                max_tokens=1000
            )
            print("âœ… SiliconFlow é…ç½®æˆåŠŸ")
        else:
            print("âŒ æœªæ‰¾åˆ° SILICONFLOW_API_KEY")
            siliconflow_model = None
        
        # åˆ›å»ºæ¨¡å‹ç®¡ç†å™¨
        print("\nğŸ¯ æ­¥éª¤3ï¼šåˆ›å»ºæ¨¡å‹ç®¡ç†å™¨")
        print("-" * 30)
        
        class ModelManager:
            """æ¨¡å‹ç®¡ç†å™¨"""
            
            def __init__(self):
                self.models = {}
                self.current_model = None
            
            def add_model(self, name: str, model, description: str = ""):
                """æ·»åŠ æ¨¡å‹"""
                self.models[name] = {
                    "model": model,
                    "description": description
                }
                if self.current_model is None:
                    self.current_model = name
                print(f"âœ… æ·»åŠ æ¨¡å‹ï¼š{name} - {description}")
            
            def switch_model(self, name: str):
                """åˆ‡æ¢æ¨¡å‹"""
                if name in self.models:
                    self.current_model = name
                    print(f"ğŸ”„ åˆ‡æ¢åˆ°æ¨¡å‹ï¼š{name}")
                    return True
                else:
                    print(f"âŒ æ¨¡å‹ä¸å­˜åœ¨ï¼š{name}")
                    return False
            
            def get_current_model(self):
                """è·å–å½“å‰æ¨¡å‹"""
                if self.current_model and self.current_model in self.models:
                    return self.models[self.current_model]["model"]
                return None
            
            def list_models(self):
                """åˆ—å‡ºæ‰€æœ‰æ¨¡å‹"""
                print("\nğŸ“‹ å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼š")
                for name, info in self.models.items():
                    current = "ğŸ‘† å½“å‰" if name == self.current_model else "  "
                    print(f"   {current} {name}: {info['description']}")
        
        # åˆå§‹åŒ–ç®¡ç†å™¨
        manager = ModelManager()
        
        if deepseek_model:
            manager.add_model("deepseek", deepseek_model, "ä¾¿å®œå®ç”¨ï¼Œä¸­æ–‡å‹å¥½")
        
        if siliconflow_model:
            manager.add_model("qwen", siliconflow_model, "å¼€æºæ¨¡å‹ï¼ŒåŠŸèƒ½å…¨é¢")
        
        manager.list_models()
        
        return manager
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥ï¼š{e}")
        return None


def model_comparison_test():
    """
    æ¨¡å‹å¯¹æ¯”æµ‹è¯•
    """
    print("\n" + "="*60)
    print("ğŸ”¬ æ¨¡å‹èƒ½åŠ›å¯¹æ¯”æµ‹è¯•")
    print("="*60)
    
    try:
        manager = setup_multi_models()
        if not manager or not manager.models:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
            return
        
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.output_parsers import StrOutputParser
        
        # æµ‹è¯•é¢˜ç›®
        test_cases = [
            {
                "name": "åˆ›æ„å†™ä½œ",
                "prompt": "å†™ä¸€ä¸ª50å­—çš„ç§‘å¹»å°æ•…äº‹",
                "è¯„ä»·æ ‡å‡†": ["åˆ›æ„æ€§", "è¯­è¨€æµç•…åº¦", "æ•…äº‹å®Œæ•´æ€§"]
            },
            {
                "name": "é€»è¾‘æ¨ç†", 
                "prompt": "å°æ˜æ¯”å°çº¢é«˜ï¼Œå°çº¢æ¯”å°æé«˜ï¼Œé—®è°æœ€é«˜ï¼Ÿè¯·è¯´æ˜æ¨ç†è¿‡ç¨‹ã€‚",
                "è¯„ä»·æ ‡å‡†": ["é€»è¾‘æ­£ç¡®æ€§", "æ¨ç†æ¸…æ™°åº¦"]
            },
            {
                "name": "ä»£ç ç”Ÿæˆ",
                "prompt": "å†™ä¸€ä¸ªPythonå‡½æ•°ï¼Œè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬né¡¹",
                "è¯„ä»·æ ‡å‡†": ["ä»£ç æ­£ç¡®æ€§", "æ•ˆç‡", "å¯è¯»æ€§"]
            }
        ]
        
        # åˆ›å»ºæµ‹è¯•é“¾
        template = ChatPromptTemplate.from_template("{prompt}")
        parser = StrOutputParser()
        
        print("ğŸ§ª å¼€å§‹å¯¹æ¯”æµ‹è¯•...")
        
        for test_case in test_cases:
            print(f"\nğŸ“ æµ‹è¯•é¡¹ç›®ï¼š{test_case['name']}")
            print(f"é¢˜ç›®ï¼š{test_case['prompt']}")
            print("-" * 50)
            
            for model_name in manager.models.keys():
                manager.switch_model(model_name)
                model = manager.get_current_model()
                
                if model:
                    chain = template | model | parser
                    try:
                        result = chain.invoke({"prompt": test_case['prompt']})
                        print(f"\nğŸ¤– {model_name} å›ç­”ï¼š")
                        print(result[:200] + "..." if len(result) > 200 else result)
                    except Exception as e:
                        print(f"\nâŒ {model_name} è°ƒç”¨å¤±è´¥ï¼š{e}")
            
            print("\n" + "="*50)
        
        print("\nğŸ’¡ å¯¹æ¯”å»ºè®®ï¼š")
        print("   ğŸ“Š æ ¹æ®æµ‹è¯•ç»“æœé€‰æ‹©æœ€é€‚åˆä½ ä»»åŠ¡çš„æ¨¡å‹")
        print("   ğŸ’° è€ƒè™‘æˆæœ¬å› ç´ ï¼Œæ—¥å¸¸ä½¿ç”¨æ¨èä¾¿å®œçš„æ¨¡å‹")
        print("   ğŸ¯ é‡è¦ä»»åŠ¡ä½¿ç”¨æ•ˆæœæœ€å¥½çš„æ¨¡å‹")
        
    except Exception as e:
        print(f"âŒ å¯¹æ¯”æµ‹è¯•å¤±è´¥ï¼š{e}")


def smart_model_router():
    """
    æ™ºèƒ½æ¨¡å‹è·¯ç”±
    """
    print("\n" + "="*60)
    print("ğŸ§  æ™ºèƒ½æ¨¡å‹è·¯ç”±ï¼šæ ¹æ®ä»»åŠ¡è‡ªåŠ¨é€‰æ‹©æ¨¡å‹")
    print("="*60)
    
    try:
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.output_parsers import StrOutputParser
        from langchain_core.runnables import RunnableLambda
        
        class SmartRouter:
            """æ™ºèƒ½æ¨¡å‹è·¯ç”±å™¨"""
            
            def __init__(self, manager):
                self.manager = manager
                self.routing_rules = {
                    "creative": "deepseek",  # åˆ›æ„ä»»åŠ¡ç”¨ DeepSeek
                    "coding": "qwen",        # ç¼–ç¨‹ä»»åŠ¡ç”¨ Qwen
                    "analysis": "deepseek",  # åˆ†æä»»åŠ¡ç”¨ DeepSeek
                    "translation": "qwen",   # ç¿»è¯‘ä»»åŠ¡ç”¨ Qwen
                    "default": "deepseek"    # é»˜è®¤ä½¿ç”¨ DeepSeek
                }
            
            def classify_task(self, prompt: str) -> str:
                """åˆ†ç±»ä»»åŠ¡ç±»å‹"""
                prompt_lower = prompt.lower()
                
                if any(word in prompt_lower for word in ["å†™ä½œ", "æ•…äº‹", "åˆ›æ„", "æƒ³è±¡"]):
                    return "creative"
                elif any(word in prompt_lower for word in ["ä»£ç ", "ç¼–ç¨‹", "å‡½æ•°", "python", "code"]):
                    return "coding"
                elif any(word in prompt_lower for word in ["åˆ†æ", "æ€»ç»“", "å½’çº³", "è§£é‡Š"]):
                    return "analysis"
                elif any(word in prompt_lower for word in ["ç¿»è¯‘", "translate", "è‹±æ–‡", "ä¸­æ–‡"]):
                    return "translation"
                else:
                    return "default"
            
            def route_request(self, request):
                """è·¯ç”±è¯·æ±‚åˆ°åˆé€‚çš„æ¨¡å‹"""
                prompt = request.get("prompt", "")
                task_type = self.classify_task(prompt)
                
                # é€‰æ‹©æ¨¡å‹
                model_name = self.routing_rules.get(task_type, "default")
                available_models = list(self.manager.models.keys())
                
                if model_name not in available_models:
                    model_name = available_models[0] if available_models else None
                
                if model_name:
                    self.manager.switch_model(model_name)
                    print(f"ğŸ¯ ä»»åŠ¡ç±»å‹ï¼š{task_type} â†’ é€‰æ‹©æ¨¡å‹ï¼š{model_name}")
                    return request
                else:
                    raise ValueError("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
        
        # åˆ›å»ºè·¯ç”±å™¨
        manager = setup_multi_models()
        if not manager:
            return
        
        router = SmartRouter(manager)
        
        # åˆ›å»ºæ™ºèƒ½é“¾æ¡
        template = ChatPromptTemplate.from_template("{prompt}")
        parser = StrOutputParser()
        
        def create_smart_chain():
            return (
                RunnableLambda(router.route_request) |
                template |
                RunnableLambda(lambda x: manager.get_current_model().invoke(x)) |
                parser
            )
        
        smart_chain = create_smart_chain()
        
        # æµ‹è¯•æ™ºèƒ½è·¯ç”±
        test_prompts = [
            "å†™ä¸€ä¸ªå…³äºæœºå™¨äººçš„å°æ•…äº‹",
            "å†™ä¸€ä¸ªè®¡ç®—é˜¶ä¹˜çš„Pythonå‡½æ•°", 
            "åˆ†æä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•è¶‹åŠ¿",
            "å°†'Hello World'ç¿»è¯‘æˆä¸­æ–‡"
        ]
        
        print("ğŸ§ª æµ‹è¯•æ™ºèƒ½è·¯ç”±...")
        
        for prompt in test_prompts:
            print(f"\nğŸ“ è¾“å…¥ï¼š{prompt}")
            try:
                result = smart_chain.invoke({"prompt": prompt})
                print(f"ğŸ¤– è¾“å‡ºï¼š{result[:150]}...")
            except Exception as e:
                print(f"âŒ å¤±è´¥ï¼š{e}")
        
    except Exception as e:
        print(f"âŒ æ™ºèƒ½è·¯ç”±å¤±è´¥ï¼š{e}")


def cost_optimization():
    """
    æˆæœ¬ä¼˜åŒ–ç­–ç•¥
    """
    print("\n" + "="*60)
    print("ğŸ’° æˆæœ¬ä¼˜åŒ–ç­–ç•¥")
    print("="*60)
    
    strategies = {
        "1. æ¨¡å‹åˆ†å±‚ä½¿ç”¨": [
            "æ—¥å¸¸ä»»åŠ¡ï¼šDeepSeekï¼ˆä¾¿å®œï¼‰",
            "é‡è¦ä»»åŠ¡ï¼šGPT-4ï¼ˆæ•ˆæœå¥½ï¼‰",
            "å¤§æ‰¹é‡ï¼šå¼€æºæ¨¡å‹ï¼ˆå…è´¹ï¼‰"
        ],
        
        "2. æç¤ºä¼˜åŒ–": [
            "ç²¾ç®€æç¤ºè¯ï¼Œå‡å°‘è¾“å…¥ tokens",
            "è®¾ç½®åˆç†çš„ max_tokens é™åˆ¶", 
            "ä½¿ç”¨ç¼“å­˜é¿å…é‡å¤è°ƒç”¨"
        ],
        
        "3. æ‰¹é‡å¤„ç†": [
            "åˆå¹¶ç›¸ä¼¼è¯·æ±‚ä¸€èµ·å¤„ç†",
            "ä½¿ç”¨ batch æ–¹æ³•å‡å°‘ç½‘ç»œå¼€é”€",
            "å¼‚æ­¥å¤„ç†æé«˜æ•ˆç‡"
        ],
        
        "4. æ™ºèƒ½é™çº§": [
            "ä¸»æ¨¡å‹å¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢å¤‡ç”¨æ¨¡å‹",
            "æ ¹æ®ä»»åŠ¡é‡è¦æ€§é€‰æ‹©æ¨¡å‹çº§åˆ«",
            "é«˜å³°æœŸä½¿ç”¨ä¾¿å®œæ¨¡å‹"
        ]
    }
    
    for strategy, tips in strategies.items():
        print(f"\nğŸ’¡ {strategy}")
        print("-" * 30)
        for tip in tips:
            print(f"   â€¢ {tip}")
    
    print("\nğŸ“Š æˆæœ¬è®¡ç®—ç¤ºä¾‹ï¼š")
    print("-" * 30)
    
    # æˆæœ¬è®¡ç®—å™¨
    def calculate_cost(model_name: str, input_tokens: int, output_tokens: int):
        costs = {
            "deepseek": {"input": 0.0014, "output": 0.0028},
            "gpt-4": {"input": 0.21, "output": 0.42},
            "qwen": {"input": 0.12, "output": 0.12}
        }
        
        if model_name in costs:
            input_cost = (input_tokens / 1000) * costs[model_name]["input"]
            output_cost = (output_tokens / 1000) * costs[model_name]["output"]
            return input_cost + output_cost
        return 0
    
    # ç¤ºä¾‹è®¡ç®—
    scenario = {
        "ä»»åŠ¡": "1000æ¬¡å®¢æœå¯¹è¯",
        "å¹³å‡è¾“å…¥": 100,
        "å¹³å‡è¾“å‡º": 200
    }
    
    print(f"åœºæ™¯ï¼š{scenario['ä»»åŠ¡']}")
    print(f"æ¯æ¬¡å¯¹è¯ï¼š{scenario['å¹³å‡è¾“å…¥']} tokensè¾“å…¥ï¼Œ{scenario['å¹³å‡è¾“å‡º']} tokensè¾“å‡º")
    print()
    
    for model in ["deepseek", "gpt-4", "qwen"]:
        cost_per_call = calculate_cost(model, scenario['å¹³å‡è¾“å…¥'], scenario['å¹³å‡è¾“å‡º'])
        total_cost = cost_per_call * 1000
        print(f"{model:<10}: å•æ¬¡ Â¥{cost_per_call:.4f}, æ€»è®¡ Â¥{total_cost:.2f}")


def next_lesson_preview():
    """
    æ€»ç»“å’Œé¢„å‘Š
    """
    print("\n" + "="*60)
    print("ğŸ“ ç¬¬4èŠ‚æ€»ç»“ & ç¬¬5èŠ‚é¢„å‘Š")
    print("="*60)
    
    print("ğŸ‰ ç¬¬4èŠ‚ä½ æŒæ¡äº†ï¼š")
    learned = [
        "âœ… äº†è§£ä¸»æµ LLM æ¨¡å‹ç”Ÿæ€",
        "âœ… å­¦ä¼šé…ç½®å¤šä¸ªæ¨¡å‹æœåŠ¡",
        "âœ… æŒæ¡æ¨¡å‹åˆ‡æ¢å’Œç®¡ç†",
        "âœ… ç†è§£æ™ºèƒ½è·¯ç”±å’Œæˆæœ¬ä¼˜åŒ–"
    ]
    
    for item in learned:
        print(f"   {item}")
    
    print("\nğŸ“š ç¬¬5èŠ‚é¢„å‘Šï¼šã€Šè¾“å‡ºè§£æä¸æ ¼å¼åŒ–ã€‹")
    print("ä½ å°†å­¦åˆ°ï¼š")
    next_topics = [
        "ğŸ”§ ç»“æ„åŒ–è¾“å‡ºè§£æ",
        "ğŸ“‹ JSON/XML æ ¼å¼å¤„ç†",
        "ğŸ¯ è‡ªå®šä¹‰è§£æå™¨å¼€å‘",
        "ğŸ“Š æ•°æ®éªŒè¯å’Œé”™è¯¯å¤„ç†"
    ]
    
    for topic in next_topics:
        print(f"   {topic}")


def main():
    """
    ä¸»å‡½æ•°ï¼šç¬¬4èŠ‚å®Œæ•´å­¦ä¹ æµç¨‹
    """
    print("ğŸ¯ LangChain å…¥é—¨åˆ°ç²¾é€š - ç¬¬4èŠ‚")
    print("ğŸ”Œ å¤šæ¨¡å‹æ¥å…¥ä¸åˆ‡æ¢")
    print("ğŸ“š å‰ç½®ï¼šå®Œæˆç¬¬1-3èŠ‚")
    
    # 1. æ¨¡å‹ç”Ÿæ€ä»‹ç»
    explain_model_landscape()
    
    # 2. é…ç½®å¤šä¸ªæ¨¡å‹
    setup_multi_models()
    
    # 3. æ¨¡å‹å¯¹æ¯”æµ‹è¯•
    model_comparison_test()
    
    # 4. æ™ºèƒ½æ¨¡å‹è·¯ç”±
    smart_model_router()
    
    # 5. æˆæœ¬ä¼˜åŒ–
    cost_optimization()
    
    # 6. æ€»ç»“é¢„å‘Š
    next_lesson_preview()
    
    print("\n" + "="*60)
    print("ğŸ‰ ç¬¬4èŠ‚å®Œæˆï¼")
    print("ğŸš€ ä½ å·²ç»æ˜¯å¤šæ¨¡å‹ç®¡ç†ä¸“å®¶äº†ï¼")
    print("="*60)


if __name__ == "__main__":
    # æ£€æŸ¥ç¯å¢ƒ
    if not os.getenv("DEEPSEEK_API_KEY"):
        print("âš ï¸  å»ºè®®é…ç½® DEEPSEEK_API_KEY")
        print("ğŸ’¡ å¦‚éœ€é…ç½®å…¶ä»–æ¨¡å‹ï¼Œè¯·è®¾ç½®å¯¹åº”çš„ API å¯†é’¥")
    
    # è¿è¡Œä¸»ç¨‹åº
    main()
    
    print("\nğŸ”— æœ¬èŠ‚å‚è€ƒèµ„æºï¼š")
    print("   ğŸ“– å„æ¨¡å‹å®˜æ–¹æ–‡æ¡£å’Œä»·æ ¼é¡µé¢")
    print("   ğŸ’» æ¨¡å‹æ€§èƒ½å¯¹æ¯”ç½‘ç«™ï¼šhttps://chat.lmsys.org/")
    print("   ğŸ¤ ç¤¾åŒºæ¨¡å‹è¯„æµ‹å’Œæ¨è")