"""
ç¬¬10èŠ‚ï¼šå‘é‡å­˜å‚¨ä¸æ£€ç´¢
=============================================

å­¦ä¹ ç›®æ ‡ï¼š
- ç†è§£å‘é‡æ•°æ®åº“çš„åŸç†å’Œä¼˜åŠ¿
- æŒæ¡ FAISS å‘é‡å­˜å‚¨çš„ä½¿ç”¨
- å­¦ä¼šé«˜çº§æ£€ç´¢ç­–ç•¥å’ŒæŠ€å·§
- äº†è§£æ£€ç´¢æ€§èƒ½ä¼˜åŒ–æ–¹æ³•

å‰ç½®çŸ¥è¯†ï¼š
- å®Œæˆç¬¬1-9èŠ‚åŸºç¡€å†…å®¹

é‡ç‚¹æ¦‚å¿µï¼š
- å‘é‡æ•°æ®åº“ä¸“ä¸ºç›¸ä¼¼æ€§æ£€ç´¢ä¼˜åŒ–
- FAISS æ˜¯é«˜æ€§èƒ½çš„å‘é‡æ£€ç´¢åº“
- ä¸åŒæ£€ç´¢ç­–ç•¥é€‚ç”¨ä¸åŒåœºæ™¯
"""

import os
import numpy as np
from typing import List, Dict, Any, Tuple
import tempfile
import json


def explain_vector_database():
    """
    è§£é‡Šå‘é‡æ•°æ®åº“çš„æ¦‚å¿µ
    """
    print("\n" + "="*60)
    print("ğŸ—„ï¸  å‘é‡æ•°æ®åº“ï¼šä¸ºAIæ£€ç´¢è€Œç”Ÿ")
    print("="*60)
    
    print("""
ğŸª æƒ³è±¡ä¸¤ç§å•†åº—çš„åŒºåˆ«ï¼š

ä¼ ç»Ÿæ•°æ®åº“ï¼ˆå…³ç³»å‹ï¼‰ï¼š
ğŸ¬ è¶…å¸‚ï¼šæŒ‰ç±»åˆ«æ•´é½æ‘†æ”¾
ğŸ“‹ æŸ¥æ‰¾ï¼šæ ¹æ®åç§°ã€ä»·æ ¼ç­‰ç²¾ç¡®æŸ¥æ‰¾
ğŸ¯ ä¼˜åŠ¿ï¼šç»“æ„åŒ–æŸ¥è¯¢ï¼Œäº‹åŠ¡æ”¯æŒ
âŒ åŠ£åŠ¿ï¼šä¸æ”¯æŒ"ç›¸ä¼¼æ€§"æŸ¥æ‰¾

å‘é‡æ•°æ®åº“ï¼š
ğŸ¨ è‰ºæœ¯å“å¸‚åœºï¼šæŒ‰é£æ ¼ã€è‰²å½©ç›¸ä¼¼åº¦æ‘†æ”¾
ğŸ” æŸ¥æ‰¾ï¼šæ‰¾"ç±»ä¼¼è«å¥ˆé£æ ¼"çš„ç”»ä½œ
ğŸ¯ ä¼˜åŠ¿ï¼šè¯­ä¹‰ç›¸ä¼¼æ€§æ£€ç´¢
âŒ åŠ£åŠ¿ï¼šä¸é€‚åˆç²¾ç¡®æŸ¥è¯¢

ğŸš€ å‘é‡æ•°æ®åº“çš„æ ¸å¿ƒä¼˜åŠ¿ï¼š
- âš¡ é«˜é€Ÿæ£€ç´¢ï¼šä¸“é—¨ä¸ºå‘é‡ä¼˜åŒ–
- ğŸ“ˆ å¯æ‰©å±•ï¼šæ”¯æŒç™¾ä¸‡çº§å‘é‡
- ğŸ¯ ç²¾ç¡®åº¦é«˜ï¼šå¤šç§ç›¸ä¼¼åº¦ç®—æ³•
- ğŸ’¾ å†…å­˜ä¼˜åŒ–ï¼šå‹ç¼©å­˜å‚¨æŠ€æœ¯
    """)
    
    print("ğŸ“Š ä¸»æµå‘é‡æ•°æ®åº“ï¼š")
    databases = [
        "ğŸ”¥ FAISSï¼šFacebookå¼€æºï¼Œæ€§èƒ½æœ€å¼º",
        "ğŸŒŠ Weaviateï¼šå¼€æºå›¾å‘é‡æ•°æ®åº“",
        "ğŸ“Œ Pineconeï¼šäº‘æœåŠ¡ï¼Œæ˜“äºä½¿ç”¨",
        "ğŸŒŸ Qdrantï¼šRustç¼–å†™ï¼Œé«˜æ€§èƒ½",
        "ğŸ” Milvusï¼šåˆ†å¸ƒå¼å‘é‡æ•°æ®åº“"
    ]
    
    for db in databases:
        print(f"   {db}")


def faiss_basic_demo():
    """
    FAISS åŸºç¡€ä½¿ç”¨æ¼”ç¤º
    """
    print("\n" + "="*60)
    print("âš¡ FAISS å‘é‡å­˜å‚¨åŸºç¡€")
    print("="*60)
    
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„åŒ…
        print("ğŸ”§ ç¯å¢ƒæ£€æŸ¥...")
        try:
            import faiss
            print("âœ… FAISS å·²å®‰è£…")
        except ImportError:
            print("âŒ FAISS æœªå®‰è£…ï¼Œè¯·æ‰§è¡Œï¼špip install faiss-cpu")
            print("ğŸ’¡ ç»§ç»­ä½¿ç”¨æ¨¡æ‹Ÿæ¼”ç¤º...")
            faiss = None
        
        print("\nğŸ¯ FAISS æ ¸å¿ƒæ¦‚å¿µ")
        print("-" * 30)
        
        # åˆ›å»ºç¤ºä¾‹å‘é‡æ•°æ®
        dimension = 128  # å‘é‡ç»´åº¦
        n_vectors = 1000  # å‘é‡æ•°é‡
        
        print(f"ğŸ“Š åˆ›å»ºæµ‹è¯•æ•°æ®ï¼š{n_vectors}ä¸ª{dimension}ç»´å‘é‡")
        
        if faiss:
            # çœŸå®çš„FAISSæ¼”ç¤º
            np.random.seed(42)
            vectors = np.random.random((n_vectors, dimension)).astype('float32')
            
            print("\nğŸ—ï¸  æ„å»ºFAISSç´¢å¼•...")
            
            # 1. åˆ›å»ºç´¢å¼•
            index = faiss.IndexFlatL2(dimension)  # L2è·ç¦»ç´¢å¼•
            print(f"   ç´¢å¼•ç±»å‹ï¼š{type(index).__name__}")
            print(f"   æ˜¯å¦è®­ç»ƒï¼š{index.is_trained}")
            
            # 2. æ·»åŠ å‘é‡
            index.add(vectors)
            print(f"   å·²æ·»åŠ å‘é‡æ•°ï¼š{index.ntotal}")
            
            # 3. æœç´¢æµ‹è¯•
            print("\nğŸ” å‘é‡æ£€ç´¢æµ‹è¯•...")
            k = 5  # è¿”å›å‰5ä¸ªæœ€ç›¸ä¼¼çš„
            query_vector = np.random.random((1, dimension)).astype('float32')
            
            distances, indices = index.search(query_vector, k)
            
            print(f"   æŸ¥è¯¢å‘é‡ç»´åº¦ï¼š{query_vector.shape}")
            print(f"   è¿”å›ç»“æœæ•°ï¼š{len(indices[0])}")
            print(f"   ç›¸ä¼¼å‘é‡ç´¢å¼•ï¼š{indices[0]}")
            print(f"   å¯¹åº”è·ç¦»ï¼š{distances[0]}")
            
            # 4. ç´¢å¼•ç»Ÿè®¡
            print(f"\nğŸ“Š ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯ï¼š")
            print(f"   æ€»å‘é‡æ•°ï¼š{index.ntotal}")
            print(f"   å‘é‡ç»´åº¦ï¼š{index.d}")
            print(f"   ç´¢å¼•å¤§å°ï¼šçº¦{index.ntotal * dimension * 4 / 1024 / 1024:.1f}MB")
            
        else:
            # æ¨¡æ‹Ÿæ¼”ç¤º
            print("ğŸ­ æ¨¡æ‹ŸFAISSå·¥ä½œæµç¨‹...")
            
            class MockFAISS:
                def __init__(self, dimension):
                    self.dimension = dimension
                    self.vectors = []
                    self.ntotal = 0
                
                def add(self, vectors):
                    self.vectors.extend(vectors.tolist())
                    self.ntotal = len(self.vectors)
                
                def search(self, query, k):
                    # ç®€å•çš„æ¨¡æ‹Ÿæœç´¢
                    distances = np.random.random(k)
                    indices = np.random.randint(0, self.ntotal, k)
                    return distances.reshape(1, -1), indices.reshape(1, -1)
            
            # æ¨¡æ‹Ÿæ•°æ®
            vectors = np.random.random((n_vectors, dimension)).astype('float32')
            
            # æ¨¡æ‹Ÿç´¢å¼•
            mock_index = MockFAISS(dimension)
            mock_index.add(vectors)
            
            # æ¨¡æ‹Ÿæœç´¢
            query_vector = np.random.random((1, dimension)).astype('float32')
            distances, indices = mock_index.search(query_vector, 5)
            
            print(f"   æ¨¡æ‹Ÿç´¢å¼•å·²åˆ›å»ºï¼š{n_vectors}ä¸ªå‘é‡")
            print(f"   æ¨¡æ‹Ÿæœç´¢ç»“æœï¼š{indices[0]}")
            print(f"   æ¨¡æ‹Ÿè·ç¦»ï¼š{distances[0]}")
        
        print("\nâœ… FAISS çš„ä¸»è¦ç‰¹ç‚¹ï¼š")
        features = [
            "âš¡ æé€Ÿæ£€ç´¢ï¼šæ¯«ç§’çº§å“åº”",
            "ğŸ¯ å¤šç§ç®—æ³•ï¼šL2ã€ä½™å¼¦ã€å†…ç§¯ç­‰",
            "ğŸ“ˆ å¯æ‰©å±•ï¼šæ”¯æŒGPUåŠ é€Ÿ",
            "ğŸ’¾ å†…å­˜å‹å¥½ï¼šæ”¯æŒç´¢å¼•å‹ç¼©"
        ]
        
        for feature in features:
            print(f"   {feature}")
        
    except Exception as e:
        print(f"âŒ FAISSæ¼”ç¤ºå¤±è´¥ï¼š{e}")


def langchain_vectorstore_demo():
    """
    LangChain å‘é‡å­˜å‚¨æ¼”ç¤º
    """
    print("\n" + "="*60)
    print("ğŸ”— LangChain å‘é‡å­˜å‚¨é›†æˆ")
    print("="*60)
    
    try:
        print("ğŸ¯ ä½¿ç”¨ LangChain çš„å‘é‡å­˜å‚¨æŠ½è±¡")
        print("-" * 40)
        
        # æ¨¡æ‹Ÿæ–‡æ¡£æ•°æ®
        documents = [
            {"content": "LangChainæ˜¯ä¸€ä¸ªå¼ºå¤§çš„AIåº”ç”¨å¼€å‘æ¡†æ¶", "metadata": {"source": "doc1", "type": "intro"}},
            {"content": "LCELæä¾›äº†é“¾å¼è°ƒç”¨çš„ç®€æ´è¯­æ³•", "metadata": {"source": "doc2", "type": "syntax"}},
            {"content": "æç¤ºæ¨¡æ¿å¸®åŠ©æ„å»ºé«˜è´¨é‡çš„AIäº¤äº’", "metadata": {"source": "doc3", "type": "template"}},
            {"content": "å‘é‡å­˜å‚¨æ˜¯RAGç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶", "metadata": {"source": "doc4", "type": "storage"}},
            {"content": "FAISSæä¾›äº†é«˜æ€§èƒ½çš„ç›¸ä¼¼æ€§æ£€ç´¢", "metadata": {"source": "doc5", "type": "tech"}},
        ]
        
        print(f"ğŸ“„ å‡†å¤‡æ–‡æ¡£ï¼š{len(documents)}ä¸ª")
        for i, doc in enumerate(documents, 1):
            print(f"   {i}. {doc['content'][:30]}...")
        
        # æ¨¡æ‹ŸåµŒå…¥å‡½æ•°
        def mock_embed_documents(texts: List[str]) -> List[List[float]]:
            """æ¨¡æ‹Ÿæ–‡æ¡£åµŒå…¥"""
            embeddings = []
            for text in texts:
                # åŸºäºæ–‡æœ¬å†…å®¹ç”Ÿæˆç¡®å®šæ€§å‘é‡
                np.random.seed(hash(text) % 1000000)
                embedding = np.random.random(384).tolist()  # 384ç»´å‘é‡
                embeddings.append(embedding)
            return embeddings
        
        def mock_embed_query(text: str) -> List[float]:
            """æ¨¡æ‹ŸæŸ¥è¯¢åµŒå…¥"""
            np.random.seed(hash(text) % 1000000)
            return np.random.random(384).tolist()
        
        # æ¨¡æ‹Ÿå‘é‡å­˜å‚¨ç±»
        class MockVectorStore:
            def __init__(self):
                self.documents = []
                self.embeddings = []
                self.metadatas = []
            
            def add_documents(self, documents, embeddings=None):
                """æ·»åŠ æ–‡æ¡£"""
                if embeddings is None:
                    texts = [doc["content"] for doc in documents]
                    embeddings = mock_embed_documents(texts)
                
                self.documents.extend([doc["content"] for doc in documents])
                self.embeddings.extend(embeddings)
                self.metadatas.extend([doc["metadata"] for doc in documents])
                
                print(f"âœ… å·²æ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£åˆ°å‘é‡å­˜å‚¨")
            
            def similarity_search(self, query: str, k: int = 3):
                """ç›¸ä¼¼æ€§æœç´¢"""
                query_embedding = mock_embed_query(query)
                
                # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰
                similarities = []
                for i, doc_embedding in enumerate(self.embeddings):
                    # ç®€å•çš„ç‚¹ç§¯ç›¸ä¼¼åº¦
                    similarity = sum(a * b for a, b in zip(query_embedding, doc_embedding))
                    similarities.append((similarity, i))
                
                # æ’åºå¹¶è¿”å›top-k
                similarities.sort(reverse=True)
                
                results = []
                for similarity, idx in similarities[:k]:
                    results.append({
                        "content": self.documents[idx],
                        "metadata": self.metadatas[idx],
                        "similarity": similarity
                    })
                
                return results
            
            def similarity_search_with_score(self, query: str, k: int = 3):
                """å¸¦åˆ†æ•°çš„ç›¸ä¼¼æ€§æœç´¢"""
                return self.similarity_search(query, k)
        
        # åˆ›å»ºå‘é‡å­˜å‚¨
        print("\nğŸ—ï¸  åˆ›å»ºå‘é‡å­˜å‚¨...")
        vectorstore = MockVectorStore()
        
        # æ·»åŠ æ–‡æ¡£
        vectorstore.add_documents(documents)
        
        # æµ‹è¯•æ£€ç´¢
        print(f"\nğŸ” æµ‹è¯•ç›¸ä¼¼æ€§æ£€ç´¢...")
        
        test_queries = [
            "ä»€ä¹ˆæ˜¯LangChainæ¡†æ¶ï¼Ÿ",
            "å¦‚ä½•ä½¿ç”¨å‘é‡æ£€ç´¢ï¼Ÿ",
            "æç¤ºæ¨¡æ¿çš„ä½œç”¨",
            "FAISSæ€§èƒ½å¦‚ä½•ï¼Ÿ"
        ]
        
        for query in test_queries:
            print(f"\nâ“ æŸ¥è¯¢ï¼š{query}")
            results = vectorstore.similarity_search(query, k=2)
            
            print("ğŸ“‹ æ£€ç´¢ç»“æœï¼š")
            for i, result in enumerate(results, 1):
                content = result["content"][:40] + "..."
                similarity = result["similarity"]
                source = result["metadata"]["source"]
                print(f"   {i}. [{source}] {content} (ç›¸ä¼¼åº¦: {similarity:.3f})")
        
        print(f"\nâœ… LangChain å‘é‡å­˜å‚¨çš„ä¼˜åŠ¿ï¼š")
        advantages = [
            "ğŸ”§ ç»Ÿä¸€æ¥å£ï¼šæ”¯æŒå¤šç§å‘é‡æ•°æ®åº“",
            "ğŸ“š æ–‡æ¡£ç®¡ç†ï¼šè‡ªåŠ¨å¤„ç†æ–‡æœ¬å’Œå…ƒæ•°æ®",
            "ğŸ” æ£€ç´¢æ–¹æ³•ï¼šå¤šç§æœç´¢ç­–ç•¥",
            "ğŸ¯ è¿‡æ»¤åŠŸèƒ½ï¼šåŸºäºå…ƒæ•°æ®è¿‡æ»¤"
        ]
        
        for advantage in advantages:
            print(f"   {advantage}")
        
    except Exception as e:
        print(f"âŒ LangChainå‘é‡å­˜å‚¨æ¼”ç¤ºå¤±è´¥ï¼š{e}")


def advanced_retrieval_strategies():
    """
    é«˜çº§æ£€ç´¢ç­–ç•¥æ¼”ç¤º
    """
    print("\n" + "="*60)
    print("ğŸ¯ é«˜çº§æ£€ç´¢ç­–ç•¥")
    print("="*60)
    
    print("ğŸ’¡ æ£€ç´¢ç­–ç•¥å¯¹æ¯”")
    print("-" * 30)
    
    strategies = {
        "åŸºç¡€ç›¸ä¼¼æ€§æ£€ç´¢": {
            "åŸç†": "ç›´æ¥è®¡ç®—æŸ¥è¯¢ä¸æ–‡æ¡£çš„å‘é‡ç›¸ä¼¼åº¦",
            "ä¼˜ç‚¹": "ç®€å•å¿«é€Ÿï¼Œæ˜“äºå®ç°",
            "ç¼ºç‚¹": "å¯èƒ½è¿”å›é‡å¤æˆ–ç›¸å…³æ€§ä½çš„ç»“æœ",
            "é€‚ç”¨": "ç®€å•é—®ç­”ï¼Œæ–‡æ¡£ç›¸ä¼¼åº¦æŸ¥æ‰¾"
        },
        
        "MMRæ£€ç´¢": {
            "åŸç†": "åœ¨ç›¸ä¼¼æ€§å’Œå¤šæ ·æ€§ä¹‹é—´å¹³è¡¡",
            "ä¼˜ç‚¹": "ç»“æœå¤šæ ·åŒ–ï¼Œé¿å…é‡å¤ä¿¡æ¯",
            "ç¼ºç‚¹": "è®¡ç®—å¤æ‚åº¦ç¨é«˜",
            "é€‚ç”¨": "ç»¼åˆæ€§é—®é¢˜ï¼Œéœ€è¦å¤šè§’åº¦ä¿¡æ¯"
        },
        
        "æ··åˆæ£€ç´¢": {
            "åŸç†": "ç»“åˆå…³é”®è¯å’Œå‘é‡æ£€ç´¢",
            "ä¼˜ç‚¹": "ç²¾ç¡®åŒ¹é…+è¯­ä¹‰ç†è§£",
            "ç¼ºç‚¹": "éœ€è¦ç»´æŠ¤ä¸¤å¥—ç´¢å¼•",
            "é€‚ç”¨": "ä¸“ä¸šæœ¯è¯­æŸ¥æ‰¾ï¼Œç²¾ç¡®+æ¨¡ç³ŠåŒ¹é…"
        },
        
        "é‡æ’åºæ£€ç´¢": {
            "åŸç†": "å…ˆç²—æ£€ç´¢ï¼Œå†ç”¨æ¨¡å‹ç²¾ç¡®æ’åº",
            "ä¼˜ç‚¹": "æ£€ç´¢è´¨é‡æœ€é«˜",
            "ç¼ºç‚¹": "è®¡ç®—æˆæœ¬é«˜",
            "é€‚ç”¨": "é«˜è´¨é‡è¦æ±‚ï¼Œæ‰¹é‡å¤„ç†"
        }
    }
    
    for strategy, details in strategies.items():
        print(f"\nğŸ” {strategy}")
        print(f"   ğŸ“– åŸç†ï¼š{details['åŸç†']}")
        print(f"   âœ… ä¼˜ç‚¹ï¼š{details['ä¼˜ç‚¹']}")
        print(f"   âŒ ç¼ºç‚¹ï¼š{details['ç¼ºç‚¹']}")
        print(f"   ğŸ¯ é€‚ç”¨ï¼š{details['é€‚ç”¨']}")
    
    # MMRç®—æ³•æ¼”ç¤º
    print(f"\nğŸ§ª MMR (æœ€å¤§è¾¹é™…ç›¸å…³æ€§) ç®—æ³•æ¼”ç¤º")
    print("-" * 40)
    
    def mmr_search(query_embedding, doc_embeddings, documents, lambda_param=0.5, k=3):
        """MMRæ£€ç´¢ç®—æ³•"""
        
        def cosine_similarity(a, b):
            """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
            dot_product = sum(x * y for x, y in zip(a, b))
            norm_a = sum(x * x for x in a) ** 0.5
            norm_b = sum(x * x for x in b) ** 0.5
            return dot_product / (norm_a * norm_b) if norm_a * norm_b > 0 else 0
        
        selected_docs = []
        remaining_docs = list(range(len(documents)))
        
        while len(selected_docs) < k and remaining_docs:
            mmr_scores = []
            
            for i in remaining_docs:
                # ä¸æŸ¥è¯¢çš„ç›¸ä¼¼åº¦
                query_sim = cosine_similarity(query_embedding, doc_embeddings[i])
                
                # ä¸å·²é€‰æ–‡æ¡£çš„æœ€å¤§ç›¸ä¼¼åº¦
                max_sim_selected = 0
                for selected_idx in selected_docs:
                    sim = cosine_similarity(doc_embeddings[i], doc_embeddings[selected_idx])
                    max_sim_selected = max(max_sim_selected, sim)
                
                # MMRåˆ†æ•°ï¼šå¹³è¡¡ç›¸ä¼¼æ€§å’Œå¤šæ ·æ€§
                mmr_score = lambda_param * query_sim - (1 - lambda_param) * max_sim_selected
                mmr_scores.append((mmr_score, i))
            
            # é€‰æ‹©MMRåˆ†æ•°æœ€é«˜çš„æ–‡æ¡£
            mmr_scores.sort(reverse=True)
            best_idx = mmr_scores[0][1]
            selected_docs.append(best_idx)
            remaining_docs.remove(best_idx)
        
        return selected_docs
    
    # æ¨¡æ‹Ÿæ•°æ®
    np.random.seed(42)
    query_embedding = np.random.random(10).tolist()
    doc_embeddings = [np.random.random(10).tolist() for _ in range(6)]
    documents = [
        "LangChainæ¡†æ¶ä»‹ç»",
        "LangChainåŸºç¡€æ•™ç¨‹", 
        "Pythonç¼–ç¨‹å…¥é—¨",
        "æœºå™¨å­¦ä¹ æ¦‚è¿°",
        "æ·±åº¦å­¦ä¹ åŸç†",
        "ç¥ç»ç½‘ç»œç»“æ„"
    ]
    
    print("ğŸ“„ å€™é€‰æ–‡æ¡£ï¼š")
    for i, doc in enumerate(documents):
        print(f"   {i}. {doc}")
    
    # å¯¹æ¯”æ™®é€šæ£€ç´¢å’ŒMMRæ£€ç´¢
    print(f"\nğŸ” æ™®é€šç›¸ä¼¼æ€§æ£€ç´¢ vs MMRæ£€ç´¢ï¼š")
    
    # æ™®é€šæ£€ç´¢ï¼šåªæŒ‰ç›¸ä¼¼åº¦æ’åº
    def cosine_similarity(a, b):
        dot_product = sum(x * y for x, y in zip(a, b))
        norm_a = sum(x * x for x in a) ** 0.5
        norm_b = sum(x * x for x in b) ** 0.5
        return dot_product / (norm_a * norm_b) if norm_a * norm_b > 0 else 0
    
    similarities = [(cosine_similarity(query_embedding, emb), i) for i, emb in enumerate(doc_embeddings)]
    similarities.sort(reverse=True)
    normal_results = [i for _, i in similarities[:3]]
    
    # MMRæ£€ç´¢
    mmr_results = mmr_search(query_embedding, doc_embeddings, documents, lambda_param=0.7, k=3)
    
    print(f"\nğŸ“Š æ£€ç´¢ç»“æœå¯¹æ¯”ï¼š")
    print("æ™®é€šæ£€ç´¢ç»“æœï¼š")
    for i, idx in enumerate(normal_results, 1):
        print(f"   {i}. {documents[idx]}")
    
    print("MMRæ£€ç´¢ç»“æœï¼š")
    for i, idx in enumerate(mmr_results, 1):
        print(f"   {i}. {documents[idx]}")
    
    print(f"\nğŸ’¡ MMRçš„ä¼˜åŠ¿ï¼šé€šè¿‡å¤šæ ·æ€§å‚æ•°å¹³è¡¡ç›¸å…³æ€§å’Œå¤šæ ·æ€§ï¼Œé¿å…è¿”å›è¿‡äºç›¸ä¼¼çš„æ–‡æ¡£ã€‚")


def performance_optimization():
    """
    æ£€ç´¢æ€§èƒ½ä¼˜åŒ–
    """
    print("\n" + "="*60)
    print("âš¡ æ£€ç´¢æ€§èƒ½ä¼˜åŒ–æŠ€å·§")
    print("="*60)
    
    optimization_tips = {
        "1. ç´¢å¼•ä¼˜åŒ–": [
            "é€‰æ‹©åˆé€‚çš„ç´¢å¼•ç±»å‹ï¼ˆFlat, IVF, HNSWç­‰ï¼‰",
            "è°ƒæ•´ç´¢å¼•å‚æ•°ï¼ˆnlist, M, efConstructionç­‰ï¼‰", 
            "ä½¿ç”¨GPUåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰",
            "å®šæœŸé‡å»ºç´¢å¼•ä¼˜åŒ–æ€§èƒ½"
        ],
        
        "2. å‘é‡ä¼˜åŒ–": [
            "é™ç»´æŠ€æœ¯ï¼ˆPCA, t-SNEï¼‰å‡å°‘è®¡ç®—",
            "å‘é‡é‡åŒ–å‹ç¼©å­˜å‚¨ç©ºé—´",
            "é€‰æ‹©åˆé€‚çš„å‘é‡ç»´åº¦",
            "æ‰¹é‡å¤„ç†å‘é‡æ“ä½œ"
        ],
        
        "3. æ£€ç´¢ä¼˜åŒ–": [
            "ç¼“å­˜çƒ­é—¨æŸ¥è¯¢ç»“æœ",
            "ä½¿ç”¨æ£€ç´¢æ± é™åˆ¶æœç´¢èŒƒå›´",
            "å¼‚æ­¥å¤„ç†æé«˜ååé‡",
            "åˆ†å¸ƒå¼éƒ¨ç½²å¤„ç†é«˜å¹¶å‘"
        ],
        
        "4. æ•°æ®ä¼˜åŒ–": [
            "æ–‡æ¡£å»é‡å‡å°‘å†—ä½™",
            "æ–‡æ¡£åˆ†å—ç­–ç•¥ä¼˜åŒ–",
            "å…ƒæ•°æ®ç´¢å¼•åŠ é€Ÿè¿‡æ»¤",
            "å®šæœŸæ¸…ç†æ— æ•ˆæ•°æ®"
        ]
    }
    
    for category, tips in optimization_tips.items():
        print(f"\nğŸ¯ {category}")
        print("-" * 30)
        for tip in tips:
            print(f"   ğŸ’¡ {tip}")
    
    # æ€§èƒ½æµ‹è¯•ç¤ºä¾‹
    print(f"\nğŸ§ª æ€§èƒ½æµ‹è¯•æ¨¡æ‹Ÿ")
    print("-" * 30)
    
    import time
    
    def benchmark_search(n_docs, n_queries, vector_dim):
        """æ¨¡æ‹Ÿæ£€ç´¢æ€§èƒ½æµ‹è¯•"""
        print(f"ğŸ“Š æµ‹è¯•é…ç½®ï¼š")
        print(f"   æ–‡æ¡£æ•°é‡ï¼š{n_docs:,}")
        print(f"   æŸ¥è¯¢æ•°é‡ï¼š{n_queries}")
        print(f"   å‘é‡ç»´åº¦ï¼š{vector_dim}")
        
        # æ¨¡æ‹Ÿæ•°æ®å‡†å¤‡æ—¶é—´
        prep_time = 0.1 * n_docs / 1000  # å‡è®¾æ¯1000ä¸ªæ–‡æ¡£éœ€è¦0.1ç§’
        print(f"\nâ±ï¸  æ•°æ®å‡†å¤‡æ—¶é—´ï¼š{prep_time:.2f}ç§’")
        
        # æ¨¡æ‹Ÿæ£€ç´¢æ—¶é—´
        search_time_per_query = 0.01 + (n_docs / 100000) * 0.05  # åŸºç¡€æ—¶é—´ + è§„æ¨¡å½±å“
        total_search_time = search_time_per_query * n_queries
        
        print(f"ğŸ” å¹³å‡æ£€ç´¢æ—¶é—´ï¼š{search_time_per_query*1000:.1f}æ¯«ç§’/æŸ¥è¯¢")
        print(f"ğŸ“ˆ æ€»æ£€ç´¢æ—¶é—´ï¼š{total_search_time:.2f}ç§’")
        
        # è®¡ç®—ååé‡
        qps = n_queries / total_search_time if total_search_time > 0 else float('inf')
        print(f"âš¡ æ£€ç´¢ååé‡ï¼š{qps:.1f} QPS (æŸ¥è¯¢/ç§’)")
        
        # å†…å­˜ä¼°ç®—
        memory_mb = (n_docs * vector_dim * 4) / (1024 * 1024)  # float32å 4å­—èŠ‚
        print(f"ğŸ’¾ å†…å­˜å ç”¨ï¼šçº¦{memory_mb:.1f}MB")
        
        return {
            "qps": qps,
            "avg_latency": search_time_per_query * 1000,
            "memory_mb": memory_mb
        }
    
    # ä¸åŒè§„æ¨¡çš„æ€§èƒ½æµ‹è¯•
    test_configs = [
        (1000, 100, 384),      # å°è§„æ¨¡
        (10000, 100, 384),     # ä¸­è§„æ¨¡  
        (100000, 100, 384),    # å¤§è§„æ¨¡
        (1000000, 100, 384),   # è¶…å¤§è§„æ¨¡
    ]
    
    print("ğŸ“Š ä¸åŒè§„æ¨¡æ€§èƒ½å¯¹æ¯”ï¼š")
    print("-" * 60)
    print(f"{'è§„æ¨¡':<10} {'QPS':<8} {'å»¶è¿Ÿ(ms)':<10} {'å†…å­˜(MB)':<10}")
    print("-" * 60)
    
    for n_docs, n_queries, vector_dim in test_configs:
        result = benchmark_search(n_docs, n_queries, vector_dim)
        scale = f"{n_docs//1000}K" if n_docs >= 1000 else str(n_docs)
        print(f"{scale:<10} {result['qps']:<8.1f} {result['avg_latency']:<10.1f} {result['memory_mb']:<10.1f}")
    
    print(f"\nğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®ï¼š")
    recommendations = [
        "ğŸ“ˆ å°è§„æ¨¡(<10K)ï¼šä½¿ç”¨ç®€å•çš„Flatç´¢å¼•",
        "ğŸš€ ä¸­è§„æ¨¡(10K-100K)ï¼šä½¿ç”¨IVFç´¢å¼•",
        "âš¡ å¤§è§„æ¨¡(100K+)ï¼šä½¿ç”¨HNSWç´¢å¼•",
        "ğŸ”§ è¶…å¤§è§„æ¨¡(1M+)ï¼šè€ƒè™‘åˆ†å¸ƒå¼æ–¹æ¡ˆ"
    ]
    
    for rec in recommendations:
        print(f"   {rec}")


def next_lesson_preview():
    """
    æ€»ç»“å’Œé¢„å‘Š
    """
    print("\n" + "="*60)
    print("ğŸ“ ç¬¬10èŠ‚æ€»ç»“ & åç»­å­¦ä¹ å»ºè®®")
    print("="*60)
    
    print("ğŸ‰ ç¬¬10èŠ‚ä½ æŒæ¡äº†ï¼š")
    learned = [
        "âœ… ç†è§£å‘é‡æ•°æ®åº“çš„åŸç†å’Œä¼˜åŠ¿",
        "âœ… æŒæ¡FAISSå‘é‡å­˜å‚¨çš„ä½¿ç”¨",
        "âœ… å­¦ä¼šé«˜çº§æ£€ç´¢ç­–ç•¥ï¼ˆMMRç­‰ï¼‰",
        "âœ… äº†è§£æ£€ç´¢æ€§èƒ½ä¼˜åŒ–æ–¹æ³•"
    ]
    
    for item in learned:
        print(f"   {item}")
    
    print("\nğŸŠ æ­å–œå®ŒæˆåŸºç¡€å…¥é—¨è¯¾ç¨‹ï¼")
    print("ä½ å·²ç»ä»é›¶åŸºç¡€æˆé•¿ä¸ºLangChainå…¥é—¨ä¸“å®¶ï¼")
    
    print("\nğŸš€ è¿›é˜¶å­¦ä¹ å»ºè®®ï¼š")
    next_steps = [
        "ğŸ¤– Agentå¼€å‘ï¼šæ„å»ºæ™ºèƒ½åŠ©æ‰‹",
        "ğŸ”§ å·¥å…·é›†æˆï¼šè¿æ¥å¤–éƒ¨API",
        "ğŸ“Š ç”Ÿäº§éƒ¨ç½²ï¼šæ€§èƒ½ä¼˜åŒ–å’Œç›‘æ§", 
        "ğŸ¢ ä¼ä¸šåº”ç”¨ï¼šå®é™…é¡¹ç›®å¼€å‘",
        "ğŸŒŸ å¼€æºè´¡çŒ®ï¼šå‚ä¸ç¤¾åŒºå»ºè®¾"
    ]
    
    for step in next_steps:
        print(f"   {step}")
    
    print("\nğŸ“š æ¨èèµ„æºï¼š")
    resources = [
        "ğŸ“– LangChainå®˜æ–¹æ–‡æ¡£æ·±å…¥é˜…è¯»",
        "ğŸ’» GitHubå¼€æºé¡¹ç›®å®è·µ",
        "ğŸ¥ æŠ€æœ¯ä¼šè®®å’Œåœ¨çº¿è¯¾ç¨‹",
        "ğŸ¤ åŠ å…¥å¼€å‘è€…ç¤¾åŒºè®¨è®º",
        "ğŸ—ï¸  æ„å»ºä¸ªäººé¡¹ç›®ä½œå“é›†"
    ]
    
    for resource in resources:
        print(f"   {resource}")


def main():
    """
    ä¸»å‡½æ•°ï¼šç¬¬10èŠ‚å®Œæ•´å­¦ä¹ æµç¨‹
    """
    print("ğŸ¯ LangChain å…¥é—¨åˆ°ç²¾é€š - ç¬¬10èŠ‚")
    print("ğŸ—„ï¸  å‘é‡å­˜å‚¨ä¸æ£€ç´¢")
    print("ğŸ“š å‰ç½®ï¼šå®Œæˆç¬¬1-9èŠ‚")
    
    # 1. å‘é‡æ•°æ®åº“æ¦‚å¿µ
    explain_vector_database()
    
    # 2. FAISSåŸºç¡€æ¼”ç¤º
    faiss_basic_demo()
    
    # 3. LangChainå‘é‡å­˜å‚¨
    langchain_vectorstore_demo()
    
    # 4. é«˜çº§æ£€ç´¢ç­–ç•¥
    advanced_retrieval_strategies()
    
    # 5. æ€§èƒ½ä¼˜åŒ–
    performance_optimization()
    
    # 6. æ€»ç»“å’Œä¸‹ä¸€æ­¥
    next_lesson_preview()
    
    print("\n" + "="*60)
    print("ğŸ‰ ç¬¬10èŠ‚å®Œæˆï¼")
    print("ğŸŠ æ•´ä¸ªå…¥é—¨è¯¾ç¨‹ç³»åˆ—å®Œæˆï¼")
    print("ğŸš€ æ­å–œä½ æˆä¸ºLangChainä¸“å®¶ï¼")
    print("="*60)


if __name__ == "__main__":
    # æ£€æŸ¥ç¯å¢ƒ
    if not os.getenv("DEEPSEEK_API_KEY"):
        print("âš ï¸  è¯·å…ˆé…ç½® DEEPSEEK_API_KEY")
        import getpass
        temp_key = getpass.getpass("è¯·è¾“å…¥ DeepSeek API Key: ")
        if temp_key:
            os.environ["DEEPSEEK_API_KEY"] = temp_key
    
    # è¿è¡Œä¸»ç¨‹åº
    main()
    
    print("\nğŸ”— æœ¬èŠ‚å‚è€ƒèµ„æºï¼š")
    print("   ğŸ“– FAISSå®˜æ–¹æ–‡æ¡£å’Œæ•™ç¨‹")
    print("   ğŸ’» å‘é‡æ•°æ®åº“æ€§èƒ½å¯¹æ¯”")
    print("   ğŸ—ï¸  å¤§è§„æ¨¡æ£€ç´¢ç³»ç»Ÿæ¶æ„")
    print("\nğŸ“ è¯¾ç¨‹å®Œæˆï¼æ„Ÿè°¢ä½ çš„å­¦ä¹ ï¼")