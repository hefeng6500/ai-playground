"""
ç¬¬9èŠ‚ï¼šæ–‡æ¡£å¤„ç†ä¸ RAG åŸºç¡€
=============================================

å­¦ä¹ ç›®æ ‡ï¼š
- ç†è§£ RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰çš„æ ¸å¿ƒæ¦‚å¿µ
- æŒæ¡æ–‡æ¡£åŠ è½½å’Œé¢„å¤„ç†æŠ€æœ¯
- å­¦ä¼šæ–‡æœ¬åˆ†å‰²ç­–ç•¥å’ŒæŠ€å·§
- äº†è§£å‘é‡åŒ–å’Œç›¸ä¼¼æ€§æ£€ç´¢åŸºç¡€

å‰ç½®çŸ¥è¯†ï¼š
- å®Œæˆç¬¬1-8èŠ‚åŸºç¡€å†…å®¹

é‡ç‚¹æ¦‚å¿µï¼š
- RAG = æ£€ç´¢ + ç”Ÿæˆï¼Œè®©AIæœ‰"å¤–éƒ¨è®°å¿†"
- æ–‡æ¡£é¢„å¤„ç†æ˜¯RAGç³»ç»Ÿçš„åŸºç¡€
- å¥½çš„åˆ†å‰²ç­–ç•¥å½±å“æ£€ç´¢è´¨é‡
"""

import os
from typing import List, Dict, Any
import tempfile


def explain_rag_concept():
    """
    è§£é‡Š RAG çš„æ ¸å¿ƒæ¦‚å¿µ
    """
    print("\n" + "="*60)
    print("ğŸ§  ä»€ä¹ˆæ˜¯ RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ï¼Ÿ")
    print("="*60)
    
    print("""
ğŸ“š æƒ³è±¡ä½ åœ¨å†™è®ºæ–‡ï¼š

ä¼ ç»Ÿæ–¹å¼ï¼ˆçº¯ç”Ÿæˆï¼‰ï¼š
ğŸ‘¨â€ğŸ“ å­¦ç”Ÿï¼šå‡­è®°å¿†å†™è®ºæ–‡
ğŸ§  å¤§è„‘ï¼šåªèƒ½ç”¨å·²æœ‰çŸ¥è¯†
ğŸ“ ç»“æœï¼šå¯èƒ½ä¿¡æ¯è¿‡æ—¶ã€ä¸å‡†ç¡®

RAG æ–¹å¼ï¼ˆæ£€ç´¢+ç”Ÿæˆï¼‰ï¼š
ğŸ‘¨â€ğŸ“ å­¦ç”Ÿï¼šæŸ¥é˜…èµ„æ–™åå†™è®ºæ–‡  
ğŸ“š å›¾ä¹¦é¦†ï¼šæŸ¥æ‰¾ç›¸å…³èµ„æ–™
ğŸ§  å¤§è„‘ï¼šç»“åˆèµ„æ–™å’ŒçŸ¥è¯†
ğŸ“ ç»“æœï¼šä¿¡æ¯æ›´æ–°ã€æ›´å‡†ç¡®

RAG çš„å·¥ä½œæµç¨‹ï¼š
1. ğŸ“„ æ–‡æ¡£å‡†å¤‡ï¼šå°†çŸ¥è¯†åº“æ–‡æ¡£å‘é‡åŒ–å­˜å‚¨
2. ğŸ” ç›¸å…³æ£€ç´¢ï¼šæ ¹æ®ç”¨æˆ·é—®é¢˜æ£€ç´¢ç›¸å…³æ–‡æ¡£
3. ğŸ§  å¢å¼ºç”Ÿæˆï¼šç»“åˆæ£€ç´¢ç»“æœç”Ÿæˆå›ç­”
4. ğŸ“¤ è¿”å›ç­”æ¡ˆï¼šç»™å‡ºæœ‰æ ¹æ®çš„å›ç­”
    """)
    
    print("ğŸ¯ RAG çš„ä¼˜åŠ¿ï¼š")
    advantages = [
        "ğŸ“ˆ çŸ¥è¯†æ›´æ–°ï¼šéšæ—¶æ·»åŠ æ–°æ–‡æ¡£",
        "ğŸ¯ å‡†ç¡®æ€§é«˜ï¼šåŸºäºå®é™…æ–‡æ¡£å›ç­”",
        "ğŸ” å¯è¿½æº¯ï¼šå¯ä»¥æŒ‡å‡ºä¿¡æ¯æ¥æº",
        "ğŸ’¾ æˆæœ¬ä½ï¼šä¸ç”¨é‡æ–°è®­ç»ƒæ¨¡å‹",
        "ğŸ”’ ç§æœ‰æ•°æ®ï¼šå¯ä»¥å¤„ç†ä¼ä¸šå†…éƒ¨æ–‡æ¡£"
    ]
    
    for advantage in advantages:
        print(f"   {advantage}")


def document_loading_demo():
    """
    æ–‡æ¡£åŠ è½½æ¼”ç¤º
    """
    print("\n" + "="*60)
    print("ğŸ“„ æ–‡æ¡£åŠ è½½ä¸é¢„å¤„ç†")
    print("="*60)
    
    try:
        print("ğŸ¯ æ”¯æŒçš„æ–‡æ¡£ç±»å‹")
        print("-" * 30)
        
        # æ¨¡æ‹Ÿä¸åŒç±»å‹çš„æ–‡æ¡£å†…å®¹
        documents = {
            "çº¯æ–‡æœ¬": """
LangChainæ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ç”±è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ã€‚
å®ƒæä¾›äº†ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½ï¼š
1. æ¨¡å‹é›†æˆï¼šæ”¯æŒå¤šç§LLM
2. é“¾å¼è°ƒç”¨ï¼šç»„åˆä¸åŒç»„ä»¶
3. å†…å­˜ç®¡ç†ï¼šä¿æŒå¯¹è¯çŠ¶æ€
4. å·¥å…·é›†æˆï¼šè¿æ¥å¤–éƒ¨API
            """.strip(),
            
            "Markdown": """
# LangChain ä½¿ç”¨æŒ‡å—

## å®‰è£…
```bash
pip install langchain
```

## å¿«é€Ÿå¼€å§‹
LangChain çš„æ ¸å¿ƒæ˜¯**é“¾å¼è°ƒç”¨**ï¼š
- æç¤ºæ¨¡æ¿ â†’ æ¨¡å‹ â†’ è¾“å‡ºè§£æå™¨

## ç‰¹æ€§
- âœ… æ¨¡å—åŒ–è®¾è®¡
- âœ… ä¸°å¯Œçš„ç”Ÿæ€
            """.strip(),
            
            "JSON": {
                "title": "LangChain APIæ–‡æ¡£",
                "version": "0.1.0",
                "endpoints": [
                    {"path": "/chat", "method": "POST", "description": "èŠå¤©æ¥å£"},
                    {"path": "/embed", "method": "POST", "description": "å‘é‡åŒ–æ¥å£"}
                ]
            }
        }
        
        print("ğŸ“‹ æ–‡æ¡£ç¤ºä¾‹ï¼š")
        for doc_type, content in documents.items():
            print(f"\nğŸ“ {doc_type} æ–‡æ¡£ï¼š")
            if isinstance(content, str):
                print(content[:150] + "..." if len(content) > 150 else content)
            else:
                import json
                print(json.dumps(content, ensure_ascii=False, indent=2))
        
        # æ–‡æ¡£é¢„å¤„ç†æ­¥éª¤
        print(f"\nğŸ”§ æ–‡æ¡£é¢„å¤„ç†æµç¨‹ï¼š")
        preprocessing_steps = [
            "ğŸ“¥ åŠ è½½ï¼šè¯»å–å„ç§æ ¼å¼æ–‡æ¡£",
            "ğŸ§¹ æ¸…ç†ï¼šå»é™¤æ— å…³æ ¼å¼å’Œå™ªå£°",
            "ğŸ“ æ ‡å‡†åŒ–ï¼šç»Ÿä¸€æ–‡æœ¬æ ¼å¼",
            "ğŸ·ï¸  å…ƒæ•°æ®ï¼šæå–æ–‡æ¡£ä¿¡æ¯",
            "âœ‚ï¸  åˆ†å‰²ï¼šåˆ‡åˆ†æˆåˆé€‚å¤§å°çš„å—"
        ]
        
        for step in preprocessing_steps:
            print(f"   {step}")
        
        # ç®€å•çš„æ–‡æ¡£å¤„ç†ç¤ºä¾‹
        def simple_document_processor(text: str, doc_type: str = "text"):
            """ç®€å•çš„æ–‡æ¡£å¤„ç†å™¨"""
            
            # åŸºç¡€æ¸…ç†
            if doc_type == "markdown":
                # ç§»é™¤markdownæ ‡è®°
                import re
                text = re.sub(r'#+ ', '', text)  # ç§»é™¤æ ‡é¢˜æ ‡è®°
                text = re.sub(r'\*\*(.*?)\*\*', r'\1', text)  # ç§»é™¤ç²—ä½“
                text = re.sub(r'```.*?```', '', text, flags=re.DOTALL)  # ç§»é™¤ä»£ç å—
            
            # æ ‡å‡†åŒ–æ¢è¡Œå’Œç©ºæ ¼
            text = ' '.join(text.split())
            
            # è®¡ç®—åŸºç¡€ç»Ÿè®¡
            stats = {
                "char_count": len(text),
                "word_count": len(text.split()),
                "line_count": text.count('\n') + 1
            }
            
            return {
                "processed_text": text,
                "statistics": stats,
                "doc_type": doc_type
            }
        
        print(f"\nğŸ§ª æµ‹è¯•æ–‡æ¡£å¤„ç†å™¨ï¼š")
        
        for doc_type, content in documents.items():
            if isinstance(content, str):
                result = simple_document_processor(content, doc_type.lower())
                stats = result["statistics"]
                print(f"\nğŸ“Š {doc_type} å¤„ç†ç»“æœï¼š")
                print(f"   å­—ç¬¦æ•°ï¼š{stats['char_count']}")
                print(f"   è¯æ•°ï¼š{stats['word_count']}")
                print(f"   å¤„ç†åé¢„è§ˆï¼š{result['processed_text'][:100]}...")
        
    except Exception as e:
        print(f"âŒ æ–‡æ¡£åŠ è½½æ¼”ç¤ºå¤±è´¥ï¼š{e}")


def text_splitting_demo():
    """
    æ–‡æœ¬åˆ†å‰²æ¼”ç¤º
    """
    print("\n" + "="*60)
    print("âœ‚ï¸  æ–‡æœ¬åˆ†å‰²ç­–ç•¥")
    print("="*60)
    
    print("""
ğŸ¯ ä¸ºä»€ä¹ˆéœ€è¦æ–‡æœ¬åˆ†å‰²ï¼Ÿ

å¤§æ–‡æ¡£çš„é—®é¢˜ï¼š
- ğŸ“ è¶…å‡ºæ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶
- ğŸ¯ æ£€ç´¢ç²¾åº¦ä¸‹é™ï¼ˆä¿¡æ¯å¤ªæ‚ï¼‰
- ğŸ’¾ å‘é‡å­˜å‚¨æ•ˆç‡ä½
- ğŸ” ç›¸å…³æ€§åˆ¤æ–­å›°éš¾

åˆ†å‰²çš„ç›®æ ‡ï¼š
- ğŸ“¦ åˆé€‚å¤§å°ï¼šä¸è¶…è¿‡æ¨¡å‹é™åˆ¶
- ğŸ§© è¯­ä¹‰å®Œæ•´ï¼šä¿æŒå†…å®¹è¿è´¯æ€§
- ğŸ”— é‡å å¤„ç†ï¼šé¿å…ä¿¡æ¯æ–­è£‚
- ğŸ“Š å‡åŒ€åˆ†å¸ƒï¼šå¤§å°å°½é‡ä¸€è‡´
    """)
    
    try:
        # æµ‹è¯•æ–‡æ¡£
        sample_text = """
        äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹å¯ä»¥åˆ†ä¸ºå‡ ä¸ªé‡è¦é˜¶æ®µã€‚

        ç¬¬ä¸€é˜¶æ®µï¼šç¬¦å·ä¸»ä¹‰æ—¶æœŸï¼ˆ1950-1980å¹´ä»£ï¼‰
        è¿™ä¸ªæ—¶æœŸçš„AIä¸»è¦åŸºäºé€»è¾‘æ¨ç†å’Œç¬¦å·æ“ä½œã€‚ç§‘å­¦å®¶ä»¬è®¤ä¸ºæ™ºèƒ½å¯ä»¥é€šè¿‡ç¬¦å·å’Œè§„åˆ™æ¥è¡¨ç¤ºã€‚
        ä»£è¡¨æ€§æˆæœåŒ…æ‹¬ä¸“å®¶ç³»ç»Ÿå’ŒçŸ¥è¯†å›¾è°±ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•åœ¨å¤„ç†ä¸ç¡®å®šæ€§å’Œå¸¸è¯†æ¨ç†æ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚

        ç¬¬äºŒé˜¶æ®µï¼šè¿æ¥ä¸»ä¹‰å…´èµ·ï¼ˆ1980-2000å¹´ä»£ï¼‰
        ç¥ç»ç½‘ç»œçš„é‡æ–°å…´èµ·æ ‡å¿—ç€è¿™ä¸ªæ—¶æœŸçš„å¼€å§‹ã€‚å¤šå±‚æ„ŸçŸ¥æœºã€åå‘ä¼ æ’­ç®—æ³•çš„å‘æ˜ä½¿å¾—ç¥ç»ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ å¤æ‚çš„æ¨¡å¼ã€‚
        ä½†ç”±äºè®¡ç®—èƒ½åŠ›å’Œæ•°æ®çš„é™åˆ¶ï¼Œç¥ç»ç½‘ç»œçš„å‘å±•ä¸€åº¦é™·å…¥ä½è°·ã€‚

        ç¬¬ä¸‰é˜¶æ®µï¼šæ·±åº¦å­¦ä¹ é©å‘½ï¼ˆ2000å¹´ä»£è‡³ä»Šï¼‰
        éšç€å¤§æ•°æ®ã€äº‘è®¡ç®—å’ŒGPUè®¡ç®—èƒ½åŠ›çš„æå‡ï¼Œæ·±åº¦å­¦ä¹ è¿æ¥äº†çˆ†å‘å¼å¢é•¿ã€‚
        å·ç§¯ç¥ç»ç½‘ç»œåœ¨å›¾åƒè¯†åˆ«é¢†åŸŸå–å¾—çªç ´ï¼Œå¾ªç¯ç¥ç»ç½‘ç»œåœ¨åºåˆ—å¤„ç†æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚
        Transformeræ¶æ„çš„æå‡ºæ›´æ˜¯æ¨åŠ¨äº†è‡ªç„¶è¯­è¨€å¤„ç†çš„é‡å¤§è¿›å±•ã€‚

        ç¬¬å››é˜¶æ®µï¼šå¤§æ¨¡å‹æ—¶ä»£ï¼ˆ2018å¹´è‡³ä»Šï¼‰
        BERTã€GPTç­‰å¤§å‹é¢„è®­ç»ƒæ¨¡å‹çš„å‡ºç°ï¼Œæ ‡å¿—ç€AIè¿›å…¥äº†æ–°çš„æ—¶ä»£ã€‚
        è¿™äº›æ¨¡å‹é€šè¿‡åœ¨æµ·é‡æ–‡æœ¬ä¸Šé¢„è®­ç»ƒï¼Œè·å¾—äº†å¼ºå¤§çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚
        ChatGPTçš„æˆåŠŸæ›´æ˜¯å°†AIåº”ç”¨æ¨å‘äº†æ–°çš„é«˜åº¦ã€‚

        æœªæ¥å±•æœ›
        AIçš„å‘å±•ä»åœ¨åŠ é€Ÿï¼Œå¤šæ¨¡æ€AIã€é€šç”¨äººå·¥æ™ºèƒ½ç­‰æ¦‚å¿µæ­£åœ¨é€æ­¥å®ç°ã€‚
        åŒæ—¶ï¼ŒAIçš„å®‰å…¨æ€§ã€å¯è§£é‡Šæ€§å’Œä¼¦ç†é—®é¢˜ä¹Ÿè¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚
        """
        
        print("ğŸ§ª æµ‹è¯•ä¸åŒçš„åˆ†å‰²ç­–ç•¥")
        print("-" * 30)
        
        # ç­–ç•¥1ï¼šæŒ‰å­—ç¬¦é•¿åº¦åˆ†å‰²
        def split_by_chars(text: str, chunk_size: int = 200, overlap: int = 50):
            """æŒ‰å­—ç¬¦é•¿åº¦åˆ†å‰²"""
            chunks = []
            start = 0
            
            while start < len(text):
                end = start + chunk_size
                chunk = text[start:end]
                
                # å°è¯•åœ¨å¥å·å¤„åˆ†å‰²ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´
                if end < len(text) and 'ã€‚' in chunk:
                    last_period = chunk.rfind('ã€‚')
                    if last_period > chunk_size // 2:  # ç¡®ä¿å—ä¸ä¼šå¤ªå°
                        chunk = chunk[:last_period + 1]
                        end = start + last_period + 1
                
                chunks.append(chunk.strip())
                start = end - overlap  # é‡å 
            
            return chunks
        
        # ç­–ç•¥2ï¼šæŒ‰æ®µè½åˆ†å‰²
        def split_by_paragraphs(text: str, max_chars: int = 300):
            """æŒ‰æ®µè½åˆ†å‰²ï¼Œåˆå¹¶å°æ®µè½"""
            paragraphs = text.split('\n\n')
            chunks = []
            current_chunk = ""
            
            for para in paragraphs:
                para = para.strip()
                if not para:
                    continue
                
                if len(current_chunk) + len(para) <= max_chars:
                    current_chunk += para + "\n\n"
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = para + "\n\n"
            
            if current_chunk:
                chunks.append(current_chunk.strip())
            
            return chunks
        
        # ç­–ç•¥3ï¼šæŒ‰è¯­ä¹‰åˆ†å‰²ï¼ˆç®€åŒ–ç‰ˆï¼‰
        def split_by_semantic(text: str, max_chars: int = 400):
            """æŒ‰è¯­ä¹‰å•å…ƒåˆ†å‰²"""
            # æŒ‰æ ‡é¢˜å’Œæ®µè½åˆ†å‰²
            sections = []
            current_section = ""
            
            lines = text.split('\n')
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # æ£€æµ‹æ ‡é¢˜ï¼ˆåŒ…å«"é˜¶æ®µ"ã€"æ—¶æœŸ"ç­‰å…³é”®è¯ï¼‰
                if any(keyword in line for keyword in ['é˜¶æ®µ', 'æ—¶æœŸ', 'å±•æœ›']):
                    if current_section:
                        sections.append(current_section.strip())
                    current_section = line + "\n"
                else:
                    current_section += line + "\n"
            
            if current_section:
                sections.append(current_section.strip())
            
            return sections
        
        # æµ‹è¯•ä¸‰ç§ç­–ç•¥
        strategies = {
            "å­—ç¬¦åˆ†å‰²": split_by_chars,
            "æ®µè½åˆ†å‰²": split_by_paragraphs,
            "è¯­ä¹‰åˆ†å‰²": split_by_semantic
        }
        
        for strategy_name, split_func in strategies.items():
            print(f"\nğŸ“Š {strategy_name}ç»“æœï¼š")
            chunks = split_func(sample_text)
            
            print(f"   åˆ†å—æ•°é‡ï¼š{len(chunks)}")
            for i, chunk in enumerate(chunks, 1):
                char_count = len(chunk)
                preview = chunk.replace('\n', ' ')[:50] + "..."
                print(f"   å—{i}: {char_count}å­—ç¬¦ - {preview}")
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            chunk_sizes = [len(chunk) for chunk in chunks]
            avg_size = sum(chunk_sizes) / len(chunk_sizes)
            print(f"   å¹³å‡å¤§å°ï¼š{avg_size:.0f}å­—ç¬¦")
            print(f"   å¤§å°èŒƒå›´ï¼š{min(chunk_sizes)}-{max(chunk_sizes)}å­—ç¬¦")
        
        print(f"\nğŸ’¡ åˆ†å‰²ç­–ç•¥é€‰æ‹©å»ºè®®ï¼š")
        suggestions = [
            "ğŸ“ æŠ€æœ¯æ–‡æ¡£ï¼šæŒ‰æ®µè½æˆ–è¯­ä¹‰åˆ†å‰²",
            "ğŸ“° æ–°é—»æ–‡ç« ï¼šæŒ‰å­—ç¬¦é•¿åº¦åˆ†å‰²",
            "ğŸ“š å­¦æœ¯è®ºæ–‡ï¼šæŒ‰ç« èŠ‚å’Œæ®µè½åˆ†å‰²",
            "ğŸ’¬ å¯¹è¯è®°å½•ï¼šæŒ‰å¯¹è¯è½®æ¬¡åˆ†å‰²",
            "ğŸ“‹ åˆ—è¡¨æ•°æ®ï¼šæŒ‰æ¡ç›®åˆ†å‰²"
        ]
        
        for suggestion in suggestions:
            print(f"   {suggestion}")
        
    except Exception as e:
        print(f"âŒ æ–‡æœ¬åˆ†å‰²æ¼”ç¤ºå¤±è´¥ï¼š{e}")


def vectorization_basics():
    """
    å‘é‡åŒ–åŸºç¡€æ¦‚å¿µ
    """
    print("\n" + "="*60)
    print("ğŸ”¢ å‘é‡åŒ–ï¼šæ–‡æœ¬å˜æ•°å­—")
    print("="*60)
    
    print("""
ğŸ¯ ä»€ä¹ˆæ˜¯å‘é‡åŒ–ï¼Ÿ

æ–‡æœ¬ â†’ æ•°å­—å‘é‡çš„è½¬æ¢è¿‡ç¨‹ï¼š

ä¾‹å­ï¼š
"æˆ‘å–œæ¬¢è‹¹æœ" â†’ [0.2, -0.1, 0.8, 0.3, ...]
"è‹¹æœå¾ˆç”œ" â†’ [0.3, -0.2, 0.7, 0.4, ...]

ç›¸ä¼¼çš„æ–‡æœ¬ â†’ ç›¸ä¼¼çš„å‘é‡ï¼š
- ä½™å¼¦ç›¸ä¼¼åº¦ï¼šè®¡ç®—å‘é‡å¤¹è§’
- æ¬§å‡ é‡Œå¾—è·ç¦»ï¼šè®¡ç®—å‘é‡è·ç¦»
- ç‚¹ç§¯ï¼šè®¡ç®—å‘é‡ç›¸å…³æ€§

ğŸ” æ£€ç´¢åŸç†ï¼š
1. é—®é¢˜å‘é‡åŒ–ï¼šç”¨æˆ·é—®é¢˜ â†’ å‘é‡
2. è®¡ç®—ç›¸ä¼¼åº¦ï¼šä¸æ‰€æœ‰æ–‡æ¡£å‘é‡æ¯”è¾ƒ
3. æ’åºè¿”å›ï¼šæœ€ç›¸ä¼¼çš„æ–‡æ¡£æ’åœ¨å‰é¢
    """)
    
    # ç®€å•çš„å‘é‡åŒ–æ¼”ç¤ºï¼ˆä½¿ç”¨éšæœºå‘é‡æ¨¡æ‹Ÿï¼‰
    import random
    import math
    
    def simple_vectorize(text: str, dimension: int = 5) -> List[float]:
        """ç®€å•çš„æ–‡æœ¬å‘é‡åŒ–ï¼ˆä»…æ¼”ç¤ºç”¨ï¼‰"""
        # åŸºäºæ–‡æœ¬å†…å®¹ç”Ÿæˆç¡®å®šæ€§å‘é‡
        random.seed(hash(text) % 1000000)
        return [random.uniform(-1, 1) for _ in range(dimension)]
    
    def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        norm1 = math.sqrt(sum(a * a for a in vec1))
        norm2 = math.sqrt(sum(a * a for a in vec2))
        
        if norm1 == 0 or norm2 == 0:
            return 0
        
        return dot_product / (norm1 * norm2)
    
    print("ğŸ§ª ç®€å•å‘é‡åŒ–æ¼”ç¤ºï¼š")
    print("-" * 30)
    
    # æµ‹è¯•æ–‡æ¡£
    documents = [
        "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯",
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦ç»„æˆéƒ¨åˆ†", 
        "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œè®­ç»ƒ",
        "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆå‡ºé—¨æ•£æ­¥",
        "æˆ‘å–œæ¬¢åƒæ°´æœï¼Œç‰¹åˆ«æ˜¯è‹¹æœå’Œé¦™è•‰"
    ]
    
    # å‘é‡åŒ–æ‰€æœ‰æ–‡æ¡£
    doc_vectors = {}
    for i, doc in enumerate(documents):
        vec = simple_vectorize(doc)
        doc_vectors[f"æ–‡æ¡£{i+1}"] = {"text": doc, "vector": vec}
        print(f"ğŸ“„ æ–‡æ¡£{i+1}: {doc}")
        print(f"   å‘é‡: [{', '.join(f'{x:.2f}' for x in vec)}]")
    
    # æµ‹è¯•æŸ¥è¯¢
    print(f"\nğŸ” ç›¸ä¼¼æ€§æ£€ç´¢æµ‹è¯•ï¼š")
    
    queries = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "ç¥ç»ç½‘ç»œå¦‚ä½•å·¥ä½œï¼Ÿ",
        "å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
    ]
    
    for query in queries:
        query_vector = simple_vectorize(query)
        print(f"\nâ“ æŸ¥è¯¢: {query}")
        print(f"   æŸ¥è¯¢å‘é‡: [{', '.join(f'{x:.2f}' for x in query_vector)}]")
        
        # è®¡ç®—ä¸æ‰€æœ‰æ–‡æ¡£çš„ç›¸ä¼¼åº¦
        similarities = []
        for doc_name, doc_info in doc_vectors.items():
            similarity = cosine_similarity(query_vector, doc_info["vector"])
            similarities.append((doc_name, similarity, doc_info["text"]))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        print("   ğŸ“Š æ£€ç´¢ç»“æœï¼ˆæŒ‰ç›¸ä¼¼åº¦æ’åºï¼‰ï¼š")
        for rank, (doc_name, sim, text) in enumerate(similarities[:3], 1):
            print(f"      {rank}. {doc_name}: {sim:.3f} - {text}")
    
    print(f"\nâœ… å‘é‡åŒ–çš„å…³é”®è¦ç‚¹ï¼š")
    key_points = [
        "ğŸ¯ è¯­ä¹‰ç†è§£ï¼šç›¸ä¼¼å«ä¹‰çš„æ–‡æœ¬å‘é‡æ¥è¿‘",
        "ğŸ“Š æ•°å€¼è®¡ç®—ï¼šå¯ä»¥è¿›è¡Œæ•°å­¦è¿ç®—",
        "ğŸ” å¿«é€Ÿæ£€ç´¢ï¼šé€šè¿‡å‘é‡è¿ç®—å¿«é€Ÿæ‰¾åˆ°ç›¸å…³å†…å®¹",
        "ğŸ“ˆ å¯æ‰©å±•ï¼šæ”¯æŒæµ·é‡æ–‡æ¡£çš„é«˜æ•ˆæ£€ç´¢"
    ]
    
    for point in key_points:
        print(f"   {point}")


def basic_rag_demo():
    """
    åŸºç¡€ RAG ç³»ç»Ÿæ¼”ç¤º
    """
    print("\n" + "="*60)
    print("ğŸ§  åŸºç¡€ RAG ç³»ç»Ÿæ„å»º")
    print("="*60)
    
    try:
        from langchain_openai import ChatOpenAI
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.output_parsers import StrOutputParser
        
        api_key = os.getenv("DEEPSEEK_API_KEY")
        if not api_key:
            print("âŒ è¯·å…ˆé…ç½® DEEPSEEK_API_KEY")
            return
        
        print("ğŸ¯ æ„å»ºç®€å•çš„RAGç³»ç»Ÿ")
        print("-" * 30)
        
        # çŸ¥è¯†åº“æ–‡æ¡£
        knowledge_base = [
            {
                "id": "doc1",
                "title": "LangChainç®€ä»‹",
                "content": "LangChainæ˜¯ä¸€ä¸ªç”¨äºå¼€å‘è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ï¼Œæä¾›äº†æ¨¡å‹é›†æˆã€é“¾å¼è°ƒç”¨ã€å†…å­˜ç®¡ç†ç­‰åŠŸèƒ½ã€‚"
            },
            {
                "id": "doc2", 
                "title": "LCELè¯­æ³•",
                "content": "LCELï¼ˆLangChain Expression Languageï¼‰ä½¿ç”¨ç®¡é“æ“ä½œç¬¦|è¿æ¥ä¸åŒç»„ä»¶ï¼Œå¦‚prompt | model | parserã€‚"
            },
            {
                "id": "doc3",
                "title": "æç¤ºæ¨¡æ¿",
                "content": "æç¤ºæ¨¡æ¿ç”¨äºæ„å»ºå‘é€ç»™è¯­è¨€æ¨¡å‹çš„æ¶ˆæ¯ï¼Œæ”¯æŒå˜é‡æ’å€¼å’Œå¤æ‚çš„å¯¹è¯æ ¼å¼ã€‚"
            },
            {
                "id": "doc4",
                "title": "è¾“å‡ºè§£æ",
                "content": "è¾“å‡ºè§£æå™¨å°†æ¨¡å‹çš„åŸå§‹è¾“å‡ºè½¬æ¢ä¸ºç»“æ„åŒ–æ•°æ®ï¼Œå¦‚JSONã€åˆ—è¡¨ç­‰æ ¼å¼ã€‚"
            }
        ]
        
        # ç®€å•çš„æ£€ç´¢å‡½æ•°
        def simple_retriever(query: str, top_k: int = 2):
            """ç®€å•çš„å…³é”®è¯æ£€ç´¢å™¨"""
            scores = []
            query_lower = query.lower()
            
            for doc in knowledge_base:
                score = 0
                content_lower = doc["content"].lower()
                title_lower = doc["title"].lower()
                
                # ç®€å•çš„å…³é”®è¯åŒ¹é…è¯„åˆ†
                for word in query_lower.split():
                    if word in title_lower:
                        score += 2  # æ ‡é¢˜åŒ¹é…æƒé‡æ›´é«˜
                    if word in content_lower:
                        score += 1
                
                scores.append((score, doc))
            
            # æŒ‰åˆ†æ•°æ’åºï¼Œè¿”å›å‰kä¸ª
            scores.sort(key=lambda x: x[0], reverse=True)
            return [doc for score, doc in scores[:top_k] if score > 0]
        
        # åˆ›å»ºRAGæç¤ºæ¨¡æ¿
        rag_prompt = ChatPromptTemplate.from_template("""
åŸºäºä»¥ä¸‹å‚è€ƒæ–‡æ¡£å›ç­”ç”¨æˆ·é—®é¢˜ï¼š

å‚è€ƒæ–‡æ¡£ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·åŸºäºå‚è€ƒæ–‡æ¡£å›ç­”ï¼Œå¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ— æ³•ä»ç»™å®šæ–‡æ¡£ä¸­æ‰¾åˆ°ç­”æ¡ˆã€‚
""")
        
        # åˆ›å»ºæ¨¡å‹å’Œè§£æå™¨
        model = ChatOpenAI(
            model="deepseek-chat",
            openai_api_key=api_key,
            openai_api_base="https://api.deepseek.com",
            temperature=0.3
        )
        parser = StrOutputParser()
        
        # RAGé“¾æ¡
        def rag_chain(question: str):
            """å®Œæ•´çš„RAGå¤„ç†æµç¨‹"""
            
            # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
            print(f"ğŸ” æ£€ç´¢ç›¸å…³æ–‡æ¡£...")
            retrieved_docs = simple_retriever(question)
            
            print(f"   æ‰¾åˆ° {len(retrieved_docs)} ä¸ªç›¸å…³æ–‡æ¡£ï¼š")
            for i, doc in enumerate(retrieved_docs, 1):
                print(f"   {i}. {doc['title']}")
            
            # 2. æ„å»ºä¸Šä¸‹æ–‡
            if retrieved_docs:
                context = "\n\n".join([
                    f"æ–‡æ¡£{i}: {doc['title']}\n{doc['content']}" 
                    for i, doc in enumerate(retrieved_docs, 1)
                ])
            else:
                context = "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚"
            
            # 3. ç”Ÿæˆå›ç­”
            print(f"ğŸ§  ç”Ÿæˆå›ç­”...")
            chain = rag_prompt | model | parser
            response = chain.invoke({
                "context": context,
                "question": question
            })
            
            return response, retrieved_docs
        
        # æµ‹è¯•RAGç³»ç»Ÿ
        print("ğŸ§ª æµ‹è¯•RAGç³»ç»Ÿï¼š")
        
        test_questions = [
            "ä»€ä¹ˆæ˜¯LangChainï¼Ÿ",
            "å¦‚ä½•ä½¿ç”¨LCELè¯­æ³•ï¼Ÿ",
            "ä»€ä¹ˆæ˜¯åŒºå—é“¾æŠ€æœ¯ï¼Ÿ",  # çŸ¥è¯†åº“ä¸­æ²¡æœ‰çš„é—®é¢˜
            "æç¤ºæ¨¡æ¿æœ‰ä»€ä¹ˆä½œç”¨ï¼Ÿ"
        ]
        
        for question in test_questions:
            print(f"\n" + "="*50)
            print(f"â“ é—®é¢˜: {question}")
            
            response, docs = rag_chain(question)
            
            print(f"ğŸ¤– å›ç­”: {response}")
            
            if docs:
                print(f"ğŸ“š å‚è€ƒæ–‡æ¡£: {[doc['title'] for doc in docs]}")
            else:
                print(f"ğŸ“š å‚è€ƒæ–‡æ¡£: æ— ç›¸å…³æ–‡æ¡£")
        
        print(f"\nâœ… RAGç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼š")
        components = [
            "ğŸ“„ çŸ¥è¯†åº“ï¼šå­˜å‚¨é¢†åŸŸæ–‡æ¡£",
            "ğŸ” æ£€ç´¢å™¨ï¼šæ‰¾åˆ°ç›¸å…³æ–‡æ¡£",
            "ğŸ§  ç”Ÿæˆå™¨ï¼šåŸºäºæ–‡æ¡£ç”Ÿæˆå›ç­”",
            "ğŸ“Š è¯„ä¼°å™¨ï¼šè¯„ä¼°å›ç­”è´¨é‡"
        ]
        
        for component in components:
            print(f"   {component}")
        
    except Exception as e:
        print(f"âŒ RAGç³»ç»Ÿæ¼”ç¤ºå¤±è´¥ï¼š{e}")


def next_lesson_preview():
    """
    æ€»ç»“å’Œé¢„å‘Š
    """
    print("\n" + "="*60)
    print("ğŸ“ ç¬¬9èŠ‚æ€»ç»“ & ç¬¬10èŠ‚é¢„å‘Š")
    print("="*60)
    
    print("ğŸ‰ ç¬¬9èŠ‚ä½ æŒæ¡äº†ï¼š")
    learned = [
        "âœ… ç†è§£RAGç³»ç»Ÿçš„æ ¸å¿ƒæ¦‚å¿µ",
        "âœ… æŒæ¡æ–‡æ¡£åŠ è½½å’Œé¢„å¤„ç†",
        "âœ… å­¦ä¼šå¤šç§æ–‡æœ¬åˆ†å‰²ç­–ç•¥",
        "âœ… äº†è§£å‘é‡åŒ–å’Œç›¸ä¼¼æ€§æ£€ç´¢",
        "âœ… æ„å»ºåŸºç¡€RAGç³»ç»Ÿ"
    ]
    
    for item in learned:
        print(f"   {item}")
    
    print("\nğŸ“š ç¬¬10èŠ‚é¢„å‘Šï¼šã€Šå‘é‡å­˜å‚¨ä¸æ£€ç´¢ã€‹")
    print("ä½ å°†å­¦åˆ°ï¼š")
    next_topics = [
        "ğŸ—„ï¸  å‘é‡æ•°æ®åº“ä»‹ç»",
        "âš¡ FAISS å‘é‡å­˜å‚¨",
        "ğŸ” é«˜çº§æ£€ç´¢ç­–ç•¥",
        "ğŸ“Š æ£€ç´¢æ€§èƒ½ä¼˜åŒ–"
    ]
    
    for topic in next_topics:
        print(f"   {topic}")


def main():
    """
    ä¸»å‡½æ•°ï¼šç¬¬9èŠ‚å®Œæ•´å­¦ä¹ æµç¨‹
    """
    print("ğŸ¯ LangChain å…¥é—¨åˆ°ç²¾é€š - ç¬¬9èŠ‚")
    print("ğŸ“„ æ–‡æ¡£å¤„ç†ä¸ RAG åŸºç¡€")
    print("ğŸ“š å‰ç½®ï¼šå®Œæˆç¬¬1-8èŠ‚")
    
    # 1. è§£é‡ŠRAGæ¦‚å¿µ
    explain_rag_concept()
    
    # 2. æ–‡æ¡£åŠ è½½æ¼”ç¤º
    document_loading_demo()
    
    # 3. æ–‡æœ¬åˆ†å‰²æ¼”ç¤º
    text_splitting_demo()
    
    # 4. å‘é‡åŒ–åŸºç¡€
    vectorization_basics()
    
    # 5. åŸºç¡€RAGç³»ç»Ÿ
    basic_rag_demo()
    
    # 6. æ€»ç»“é¢„å‘Š
    next_lesson_preview()
    
    print("\n" + "="*60)
    print("ğŸ‰ ç¬¬9èŠ‚å®Œæˆï¼")
    print("ğŸ“„ ä½ å·²ç»æŒæ¡äº†RAGåŸºç¡€æŠ€æœ¯ï¼")
    print("="*60)


if __name__ == "__main__":
    # æ£€æŸ¥ç¯å¢ƒ
    if not os.getenv("DEEPSEEK_API_KEY"):
        print("âš ï¸  è¯·å…ˆé…ç½® DEEPSEEK_API_KEY")
        import getpass
        temp_key = getpass.getpass("è¯·è¾“å…¥ DeepSeek API Key: ")
        if temp_key:
            os.environ["DEEPSEEK_API_KEY"] = temp_key
    
    # è¿è¡Œä¸»ç¨‹åº
    main()
    
    print("\nğŸ”— æœ¬èŠ‚å‚è€ƒèµ„æºï¼š")
    print("   ğŸ“– RAG è®ºæ–‡å’Œæœ€ä½³å®è·µ")
    print("   ğŸ’» æ–‡æ¡£å¤„ç†å·¥å…·åº“")
    print("   ğŸ” å‘é‡æ£€ç´¢ç®—æ³•åŸç†")