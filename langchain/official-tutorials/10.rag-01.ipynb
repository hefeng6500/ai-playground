{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 选择聊天模型",
   "id": "44be0d71c592de8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 导入必要的库\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# 检查并设置 OpenAI API 密钥\n",
    "# 如果环境变量中没有设置 API 密钥，则提示用户输入\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "# 导入 LangChain 的聊天模型初始化函数\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# 初始化 GPT-4o-mini 聊天模型\n",
    "# 这个模型将用于生成 RAG 系统的最终答案\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n"
   ],
   "id": "3497858941d0a4ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 选择嵌入模型",
   "id": "c586b4a5dc285695"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 再次导入必要的库（如果在新的 cell 中运行）\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# 确保 OpenAI API 密钥已设置\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "# 导入 OpenAI 嵌入模型\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 初始化嵌入模型\n",
    "# text-embedding-3-large 是 OpenAI 的高质量嵌入模型，用于将文本转换为向量表示\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ],
   "id": "687d27145d507d96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 向量存储",
   "id": "a0881a8c9d803553"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 导入内存向量存储\n",
     "from langchain_core.vectorstores import InMemoryVectorStore\n",
      "\n",
      "# 创建内存向量存储实例\n",
      "# InMemoryVectorStore 将文档的向量表示存储在内存中，便于快速检索\n",
      "# 传入之前创建的嵌入模型，用于将文档转换为向量\n",
      "vector_store = InMemoryVectorStore(embeddings)"
     ],
   "id": "4b36caedec237832",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### simple indexing pipeline and RAG",
   "id": "5a78dd619a1868e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 导入所需的库\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# 加载并分块博客内容\n",
    "# 使用 WebBaseLoader 从指定 URL 加载网页内容\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        # 使用 BeautifulSoup 只解析特定的 CSS 类，提高加载效率\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# 创建文本分割器\n",
    "# chunk_size=1000: 每个文档块的最大字符数\n",
    "# chunk_overlap=200: 相邻块之间的重叠字符数，确保上下文连续性\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# 将文档块索引到向量存储中\n",
    "# 这一步将文档转换为向量并存储，用于后续的相似性搜索\n",
    "_ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "# 定义问答提示模板\n",
    "# 从 LangChain Hub 拉取预定义的 RAG 提示模板\n",
    "# 注意：对于非美国的 LangSmith 端点，可能需要指定 api_url\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# 定义应用程序的状态结构\n",
    "# 使用 TypedDict 确保状态字段的类型安全\n",
    "class State(TypedDict):\n",
    "    question: str  # 用户提出的问题\n",
    "    context: List[Document]  # 检索到的相关文档\n",
    "    answer: str  # 生成的答案\n",
    "\n",
    "\n",
    "# 定义应用程序步骤\n",
    "# 检索函数：根据问题从向量存储中检索相关文档\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "# 生成函数：基于检索到的上下文生成答案\n",
    "def generate(state: State):\n",
    "    # 将所有检索到的文档内容合并为一个字符串\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    # 使用提示模板构建消息，包含问题和上下文\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    # 调用 LLM 生成答案\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# 编译应用程序并测试\n",
    "# 创建状态图，按顺序执行检索和生成步骤\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ],
   "id": "8f4fb16a1fc0be81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 测试 RAG 系统\n",
    "# 调用图执行器，传入问题并获取答案\n",
    "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "print(response[\"answer\"])"
   ],
   "id": "79125d2198624a89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LangGraph 还提供了内置工具，用于可视化应用程序的控制流程：",
   "id": "af58ec91be0ad40c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 可视化应用程序的控制流程\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# 生成并显示 Mermaid 流程图，展示 RAG 系统的执行流程\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ],
   "id": "c47e099819364334",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 详细查看 RAG 系统的输出\n",
    "result = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "\n",
    "# 打印检索到的上下文和生成的答案\n",
    "print(f\"Context: {result['context']}\\n\\n\")\n",
    "print(f\"Answer: {result['answer']}\")"
   ],
   "id": "504563bf76d5d29f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 流式输出 RAG 系统的执行过程\n",
     "# stream_mode=\"updates\" 显示每个步骤的状态更新\n",
     "for step in graph.stream(\n",
     "    {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"updates\"\n",
     "):\n",
     "    print(f\"{step}\\n\\n----------------\\n\")"
    ],
   "id": "ee96449453dc1476",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for step in graph.stream(\n",
    "    {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"updates\"\n",
    "):\n",
    "    print(f\"{step}\\n\\n----------------\\n\")"
   ],
   "id": "4ee2d6bbd12a2177",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 创建自定义的 RAG 提示模板\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 定义自定义提示模板，包含具体的回答要求\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "# 从模板字符串创建 PromptTemplate 对象\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ],
   "id": "73c36dd108e0b951",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 为文档添加元数据标签\n",
    "# 将文档分为三个部分：开头、中间、结尾\n",
    "total_documents = len(all_splits)\n",
    "third = total_documents // 3\n",
    "\n",
    "# 遍历所有文档块，为每个块添加 section 元数据\n",
    "for i, document in enumerate(all_splits):\n",
    "    if i < third:\n",
    "        document.metadata[\"section\"] = \"beginning\"\n",
    "    elif i < 2 * third:\n",
    "        document.metadata[\"section\"] = \"middle\"\n",
    "    else:\n",
    "        document.metadata[\"section\"] = \"end\"\n",
    "\n",
    "\n",
    "# 查看第一个文档的元数据\n",
    "all_splits[0].metadata"
   ],
   "id": "11bc54265e5aae2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 重新创建向量存储并添加带有元数据的文档\n",
     "from langchain_core.vectorstores import InMemoryVectorStore\n",
     "\n",
     "# 创建新的向量存储实例\n",
     "vector_store = InMemoryVectorStore(embeddings)\n",
     "# 将带有 section 元数据的文档添加到向量存储中\n",
     "_ = vector_store.add_documents(all_splits)"
    ],
   "id": "fddee66c777eb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 定义结构化搜索查询类型\n",
    "from typing import Literal\n",
    "\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "\n",
    "# 定义搜索查询的结构化输出格式\n",
    "# 这将用于让 LLM 生成结构化的搜索参数\n",
    "class Search(TypedDict):\n",
    "    \"\"\"Search query.\"\"\"\n",
    "\n",
    "    query: Annotated[str, ..., \"Search query to run.\"]  # 搜索查询字符串\n",
    "    section: Annotated[  # 要搜索的文档部分\n",
    "        Literal[\"beginning\", \"middle\", \"end\"],\n",
    "        ...,\n",
    "        \"Section to query.\",\n",
    "    ]"
   ],
   "id": "db62972391e83538",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 定义增强的状态结构，包含查询分析步骤\n",
    "class State(TypedDict):\n",
    "    question: str  # 用户问题\n",
    "    query: Search  # 结构化的搜索查询\n",
    "    context: List[Document]  # 检索到的文档\n",
    "    answer: str  # 生成的答案\n",
    "\n",
    "\n",
    "# 查询分析函数：将自然语言问题转换为结构化搜索查询\n",
    "def analyze_query(state: State):\n",
    "    # 使用结构化输出让 LLM 生成 Search 类型的查询\n",
    "    structured_llm = llm.with_structured_output(Search)\n",
    "    query = structured_llm.invoke(state[\"question\"])\n",
    "    return {\"query\": query}\n",
    "\n",
    "\n",
    "# 增强的检索函数：根据结构化查询和元数据过滤进行检索\n",
    "def retrieve(state: State):\n",
    "    query = state[\"query\"]\n",
    "    # 使用相似性搜索，并根据 section 元数据进行过滤\n",
    "    retrieved_docs = vector_store.similarity_search(\n",
    "        query[\"query\"],\n",
    "        filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],\n",
    "    )\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "# 生成函数保持不变\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# 构建增强的 RAG 图：查询分析 -> 检索 -> 生成\n",
    "graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])\n",
    "graph_builder.add_edge(START, \"analyze_query\")\n",
    "graph = graph_builder.compile()"
   ],
   "id": "9106f67820ebcdb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 可视化增强的 RAG 系统流程图\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ],
   "id": "59965403d9eafe2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 测试增强的 RAG 系统\n",
    "# 这个问题会触发 LLM 分析并选择搜索文档的 \"end\" 部分\n",
    "for step in graph.stream(\n",
    "    {\"question\": \"What does the end of the post say about Task Decomposition?\"},\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    print(f\"{step}\\n\\n----------------\\n\")"
   ],
   "id": "afbf5f7911dff346",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
