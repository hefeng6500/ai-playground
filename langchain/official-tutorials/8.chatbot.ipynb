{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b08301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917733ac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4b3691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "print(model.invoke([HumanMessage(content=\"Hi! I'm Bob\")]))\n",
    "\n",
    "# 模型本身并没有状态的概念,不记得你的名字\n",
    "print(model.invoke([HumanMessage(content=\"What's my name?\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e31ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "print(model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa24581",
   "metadata": {},
   "source": [
    "### Message persistence 消息持久化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ede23b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8524fcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea53552",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hi! I'm Bob.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24443847",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a8c87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更换 thread_id\n",
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bba13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用相同的 thread_id 继续对话\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14240fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 若要支持异步操作，请将 call_model 节点更新为异步函数，并在调用应用时使用 .ainvoke：\n",
    "\n",
    "# Async function for node:\n",
    "async def call_model(state: MessagesState):\n",
    "    response = await model.ainvoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define graph as before:\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# Async invocation:\n",
    "output = await app.ainvoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2ddcda",
   "metadata": {},
   "source": [
    "### Prompt templates 提示模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b383fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You talk like a pirate. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3db3e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627e74d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
    "query = \"Hi! I'm Jim.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8276959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fba2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05891b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入类型注解相关模块\n",
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "# 定义自定义状态类，扩展 MessagesState\n",
    "# 添加 language 字段来支持多语言功能\n",
    "class State(TypedDict):\n",
    "    # messages 字段使用 add_messages 注解，支持消息列表的自动合并\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    # language 字段用于指定回复语言\n",
    "    language: str\n",
    "\n",
    "\n",
    "# 使用自定义状态创建工作流\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "# 更新模型调用函数，支持自定义状态\n",
    "def call_model(state: State):\n",
    "    # 使用状态中的所有信息（包括 language）来格式化提示\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    # 返回消息列表（注意这里用的是列表格式）\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# 构建工作流图\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# 编译应用\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bcfffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试多语言功能\n",
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "query = \"Hi! I'm Bob.\"\n",
    "language = \"Spanish\"  # 指定回复语言为西班牙语\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "# 传入消息和语言参数\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "# 模型应该用西班牙语回复\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4c31de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 继续对话，测试记忆功能\n",
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "# 注意：这里没有传入 language 参数\n",
    "# 但由于使用了相同的 thread_id，之前的语言设置可能会被保留\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "# 模型应该记住用户名字是 Bob，并可能继续用西班牙语回复\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe79bf3",
   "metadata": {},
   "source": [
    "### Managing Conversation History 管理对话历史\n",
    "\n",
    "在长时间的对话中，消息历史可能会变得很长，这会增加成本并可能超出模型的上下文限制。\n",
    "我们可以使用消息修剪功能来管理对话历史的长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee080bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入消息修剪相关组件\n",
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "# 创建消息修剪器\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,        # 最大 token 数量\n",
    "    strategy=\"last\",      # 保留最后的消息\n",
    "    token_counter=model,  # 使用模型来计算 token 数量\n",
    "    include_system=True,  # 包含系统消息\n",
    "    allow_partial=False,  # 不允许部分消息\n",
    "    start_on=\"human\",     # 从人类消息开始\n",
    ")\n",
    "\n",
    "# 创建一个长对话历史用于测试\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "# 应用修剪器，查看保留的消息\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d25af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建带有消息修剪功能的工作流\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "# 定义带有调试信息的模型调用函数\n",
    "def call_model(state: State):\n",
    "    # 打印修剪前的消息数量\n",
    "    print(f\"Messages before trimming: {len(state['messages'])}\")\n",
    "    # 应用消息修剪\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    # 打印修剪后的消息数量\n",
    "    print(f\"Messages after trimming: {len(trimmed_messages)}\")\n",
    "    print(\"Remaining messages:\")\n",
    "    # 显示保留的消息内容\n",
    "    for msg in trimmed_messages:\n",
    "        print(f\"  {type(msg).__name__}: {msg.content}\")\n",
    "    # 使用修剪后的消息和语言信息格式化提示\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# 构建工作流图\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# 编译应用\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881fc111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试消息修剪功能\n",
    "config = {\"configurable\": {\"thread_id\": \"abc567\"}}\n",
    "query = \"What is my name?\"\n",
    "language = \"English\"\n",
    "\n",
    "# 将之前的长对话历史与新消息合并\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "# 调用应用，观察消息修剪的效果\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "# 由于消息被修剪，模型可能不记得用户的名字\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe7cb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试另一个问题，验证消息修剪的效果\n",
    "config = {\"configurable\": {\"thread_id\": \"abc678\"}}\n",
    "\n",
    "query = \"What math problem was asked?\"\n",
    "language = \"English\"\n",
    "\n",
    "# 使用相同的长对话历史\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "# 由于消息修剪，模型应该能够回答关于数学问题的询问（2+2）\n",
    "# 因为这个问题在修剪后保留的消息中\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574c0c51",
   "metadata": {},
   "source": [
    "### Streaming 流式输出\n",
    "\n",
    "流式输出允许我们实时接收模型的响应，而不是等待完整的回复。\n",
    "这对于提供更好的用户体验特别有用，特别是在生成长文本时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5323b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示流式输出功能\n",
    "config = {\"configurable\": {\"thread_id\": \"abc789\"}}\n",
    "query = \"Hi I'm Todd, please tell me a joke.\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "# 使用 stream 方法进行流式调用\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    stream_mode=\"messages\",  # 设置流模式为消息\n",
    "):\n",
    "    # 只处理 AI 消息（过滤掉其他类型的消息）\n",
    "    if isinstance(chunk, AIMessage):\n",
    "        # 实时打印每个文本块，用 | 分隔\n",
    "        print(chunk.content, end=\"|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
