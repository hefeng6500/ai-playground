{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bc583aebb7345ce",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "id": "d74cf04036e0f2ce",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# 1. langchain 的 init_chat_model 函数\n",
    "# model_provider 是 LangChain 在 v1.0 之后为了“解耦模型名称与供应商”而引入的\n",
    "model = init_chat_model(model_provider=\"openai\", model=\"gpt-3.5-turbo\")  # \n",
    "\n",
    "response = model.invoke(\"为什么鹦鹉会说话？\")\n",
    "\n",
    "print(response)\n",
    "print(response.content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ba51eb46",
   "metadata": {},
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 2. langchain_openai 的 ChatOpenAI 类\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "response = model.invoke(\"如何学习 LangChain\")\n",
    "\n",
    "print(response)\n",
    "\n",
    "print(response.content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cdadf60c9f70ad62",
   "metadata": {},
   "source": [
    "## 模型传入的参数\n",
    "\n",
    "- model\n",
    "- api_key\n",
    "- temperature\n",
    "- max_tokens\n",
    "- max-retries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925270bef16e4aa5",
   "metadata": {},
   "source": [
    "## 1. 调用"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\n",
    "  \"openai:gpt-3.5-turbo\",\n",
    "  # Kwargs passed to the model:\n",
    "  temperature=0.7,\n",
    "  timeout=30,\n",
    "  max_tokens=1000,\n",
    ")\n",
    "response = model.invoke(\"为什么鹦鹉的羽毛颜色鲜艳？\")\n",
    "print(response.content)\n"
   ],
   "id": "4119eff47fc01494",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 字典格式\n",
    "from langchain.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "conversation = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates Chinese to English.\"},\n",
    "  {\"role\": \"user\", \"content\": \"翻译：我热爱编程\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"I love programming.\"},\n",
    "  {\"role\": \"user\", \"content\": \"翻译：我喜欢构建应用程序\"}\n",
    "]\n",
    "\n",
    "response = model.invoke(conversation)\n",
    "print(response)  # AIMessage(\"I enjoy building applications.\")"
   ],
   "id": "6aa409beb768d3b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 消息格式\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "conversation = [\n",
    "  SystemMessage(content=\"You are a helpful assistant that translates Chinese to English.\"),\n",
    "  HumanMessage(content=\"翻译：我热爱编程\"),\n",
    "  AIMessage(content=\"I love programming.\"),\n",
    "  HumanMessage(content=\"翻译：我喜欢构建应用程序\")\n",
    "]\n",
    "\n",
    "response = model.invoke(conversation)\n",
    "print(response)  # AIMessage(\"I enjoy building applications.\")"
   ],
   "id": "6905199a7d6822d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 流式传输",
   "id": "9e4c3e10450ff9d6"
  },
  {
   "cell_type": "code",
   "id": "5c61bad4ba517cca",
   "metadata": {},
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\n",
    "  \"openai:gpt-3.5-turbo\",\n",
    "  streaming=True,\n",
    ")\n",
    "\n",
    "for chunk in model.stream(\"为什么鹦鹉的羽毛颜色鲜艳？\"):\n",
    "  print(chunk.text, end=\"\", flush=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7b8965f8f60d0d9b",
   "metadata": {},
   "source": [
    "full = None  # None | AIMessageChunk\n",
    "for chunk in model.stream(\"What color is the sky?\"):\n",
    "  full = chunk if full is None else full + chunk\n",
    "  print(full.text)\n",
    "\n",
    "# The\n",
    "# The sky\n",
    "# The sky is\n",
    "# The sky is typically\n",
    "# The sky is typically blue\n",
    "# ...\n",
    "\n",
    "print(full.content)\n",
    "# Usually blue during the day. That blue comes from Rayleigh scattering: sunlight hits the atmosphere and the shorter blue wavelengths scatter in all directions. The color can change: red/orange/pink at sunrise and sunset, gray on cloudy days, and dark at night (with stars visible). Want the answer for a specific time or place?"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "  \"\"\"Get the weather for a city.\"\"\"\n",
    "  return f\"The weather in {city} is sunny.\"\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "agent = create_agent(\n",
    "  model=model,\n",
    "  tools=[get_weather],\n",
    ")\n",
    "\n",
    "# 🌊 实时流式输出\n",
    "for chunk in agent.stream(\n",
    "  {\"messages\": [{\"role\": \"user\", \"content\": \"查下深圳天气，然后写首诗\"}]},\n",
    "  stream_mode=\"values\",  # 输出 AgentState 中的每个阶段\n",
    "):\n",
    "  latest_msg = chunk[\"messages\"][-1]\n",
    "  if latest_msg.content:\n",
    "    print(latest_msg.content, end=\"\", flush=True)\n"
   ],
   "id": "3474a0d3e21d5981",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8608d0b4d4126253",
   "metadata": {},
   "source": [
    "### 批量 Batch"
   ]
  },
  {
   "cell_type": "code",
   "id": "f24ff03cf16772e5",
   "metadata": {},
   "source": [
    "# 返回整个批次的最终输出\n",
    "responses = model.batch([\n",
    "  \"Why do parrots have colorful feathers?\",\n",
    "  \"How do airplanes fly?\",\n",
    "  \"What is quantum computing?\"\n",
    "])\n",
    "for response in responses:\n",
    "  print(response.content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4cb3a35a7d81904f",
   "metadata": {},
   "source": [
    "# 逐个接收每个输入生成完成时的输出.在使用 batch_as_completed() 时，结果可能会乱序到达。每个结果都包含输入索引，以便在需要时匹配以重建原始顺序。\n",
    "for response in model.batch_as_completed([\n",
    "  \"为什么鹦鹉会说话？\",\n",
    "  \"如何飞机飞行？\",\n",
    "  \"什么是量子计算？\"\n",
    "], config={\"batch_size\": 1, \"max_concurrency\": 2}):  # 批量大小为1，并发数为2\n",
    "\n",
    "  print(response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a6d175abec11c3f2",
   "metadata": {},
   "source": [
    "## 2.工具调用"
   ]
  },
  {
   "cell_type": "code",
   "id": "d214a2d8c650fcf3",
   "metadata": {},
   "source": [
    "from langchain.tools import tool\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "  \"\"\"Get the weather at a location.\"\"\"\n",
    "  return f\"It's sunny in {location}.\"\n",
    "\n",
    "\n",
    "model = init_chat_model(\n",
    "  \"openai:gpt-3.5-turbo\",\n",
    "  streaming=True,\n",
    ")\n",
    "\n",
    "model_with_tools = model.bind_tools([get_weather])\n",
    "\n",
    "response = model_with_tools.invoke(\"What's the weather like in Boston?\")\n",
    "for tool_call in response.tool_calls:\n",
    "  # View tool calls made by the model\n",
    "  print(f\"Tool: {tool_call['name']}\")\n",
    "  print(f\"Args: {tool_call['args']}\")\n",
    "\n",
    "response.pretty_print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e9324cc61d64bf2f",
   "metadata": {},
   "source": [
    "## 2.1 工具执行循环"
   ]
  },
  {
   "cell_type": "code",
   "id": "2f97b4cb81de3d7d",
   "metadata": {},
   "source": [
    "# 将（可能多个）工具绑定到模型\n",
    "model_with_tools = model.bind_tools([get_weather])\n",
    "\n",
    "# Step 1: 模型生成工具调用\n",
    "messages = [{\"role\": \"user\", \"content\": \"今天波士顿的天气如何？\"}]\n",
    "ai_msg = model_with_tools.invoke(messages)\n",
    "messages.append(ai_msg)\n",
    "\n",
    "print(messages)\n",
    "\n",
    "# Step 2: 执行工具并收集结果\n",
    "for tool_call in ai_msg.tool_calls:\n",
    "  # 使用生成的参数执行该工具\n",
    "  tool_result = get_weather.invoke(tool_call)\n",
    "  messages.append(tool_result)\n",
    "\n",
    "print(messages)\n",
    "\n",
    "# Step 3: 将结果传回模型以获得最终响应\n",
    "final_response = model_with_tools.invoke(messages)\n",
    "print(final_response.text)\n",
    "# \"The current weather in Boston is 72°F and sunny.\"\n",
    "\n",
    "final_response.pretty_print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3481e766e57a6d8e",
   "metadata": {},
   "source": [
    "2.2 强制工具调用\n",
    "\n",
    "强制使用任何工具： \n",
    "```python\n",
    "model_with_tools = model.bind_tools([tool_1], tool_choice=\"any\")\n",
    "```\n",
    "\n",
    "强制使用特定工具\n",
    "```python\n",
    "model_with_tools = model.bind_tools([tool_1], tool_choice=\"tool_1\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b8b67a8c5ef9e4",
   "metadata": {},
   "source": [
    "### 2.3 并行工具调用\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b271bd0c6e16f768",
   "metadata": {},
   "source": [
    "model_with_tools = model.bind_tools([get_weather])\n",
    "\n",
    "response = model_with_tools.invoke(\n",
    "  \"What's the weather in Boston and Tokyo?\"\n",
    ")\n",
    "\n",
    "# The model may generate multiple tool calls\n",
    "print(response.tool_calls)\n",
    "# [\n",
    "#   {'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': 'call_1'},\n",
    "#   {'name': 'get_weather', 'args': {'location': 'Tokyo'}, 'id': 'call_2'}\n",
    "# ]\n",
    "\n",
    "\n",
    "# Execute all tools (can be done in parallel with async)\n",
    "results = []\n",
    "for tool_call in response.tool_calls:\n",
    "  if tool_call['name'] == 'get_weather':\n",
    "    result = get_weather.invoke(tool_call)\n",
    "  # ...\n",
    "  results.append(result)\n",
    "\n",
    "print(results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "大多数支持工具调用的模型默认启用并行工具调用。一些模型（包括 OpenAI 和 Anthropic）允许您禁用此功能。要做到这一点，请设置 parallel_tool_calls=False\n",
    "```python\n",
    "model.bind_tools([get_weather], parallel_tool_calls=False)\n",
    "```\n"
   ],
   "id": "4e9da20fe98cb10b"
  },
  {
   "cell_type": "markdown",
   "id": "3f104d5b64ab3cd7",
   "metadata": {},
   "source": [
    "### 2.4 流式工具调用"
   ]
  },
  {
   "cell_type": "code",
   "id": "5147143df3209fe7",
   "metadata": {},
   "source": [
    "for chunk in model_with_tools.stream(\n",
    "  \"What's the weather in Boston and Tokyo?\"\n",
    "):\n",
    "  # Tool call chunks arrive progressively\n",
    "  for tool_chunk in chunk.tool_call_chunks:\n",
    "    if name := tool_chunk.get(\"name\"):\n",
    "      print(f\"Tool: {name}\")\n",
    "    if id_ := tool_chunk.get(\"id\"):\n",
    "      print(f\"ID: {id_}\")\n",
    "    if args := tool_chunk.get(\"args\"):\n",
    "      print(f\"Args: {args}\")\n",
    "\n",
    "# Output:\n",
    "# Tool: get_weather\n",
    "# ID: call_SvMlU1TVIZugrFLckFE2ceRE\n",
    "# Args: {\"lo\n",
    "# Args: catio\n",
    "# Args: n\": \"B\n",
    "# Args: osto\n",
    "# Args: n\"}\n",
    "# Tool: get_weather\n",
    "# ID: call_QMZdy6qInx13oWKE7KhuhOLR\n",
    "# Args: {\"lo\n",
    "# Args: catio\n",
    "# Args: n\": \"T\n",
    "# Args: okyo\n",
    "# Args: \"}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e469304569800b53",
   "metadata": {},
   "source": [
    "gathered = None\n",
    "for chunk in model_with_tools.stream(\"What's the weather in Boston?\"):\n",
    "  gathered = chunk if gathered is None else gathered + chunk\n",
    "  print(gathered)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a1f4cd81",
   "metadata": {},
   "source": [
    "## 3.结构树输出"
   ]
  },
  {
   "cell_type": "code",
   "id": "4599cba9",
   "metadata": {},
   "source": [
    "# 3.1 Pydantic 模型 提供了最丰富的功能集，包括字段验证、描述和嵌套结构。\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "class Movie(BaseModel):\n",
    "  \"\"\"A movie with details.\"\"\"\n",
    "  title: str = Field(..., description=\"The title of the movie\")\n",
    "  year: int = Field(..., description=\"The year the movie was released\")\n",
    "  director: str = Field(..., description=\"The director of the movie\")\n",
    "  rating: float = Field(..., description=\"The movie's rating out of 10\")\n",
    "\n",
    "\n",
    "model = init_chat_model(\"openai:gpt-5\")\n",
    "\n",
    "# include_raw=True 会在响应中包含原始文本，便于调试和记录。\n",
    "model_with_structure = model.with_structured_output(Movie, include_raw=True)\n",
    "\n",
    "response = model_with_structure.invoke(\"提供电影《Inception》的信息。\")\n",
    "\n",
    "print(response)  # Movie(title=\"Inception\", year=2010, director=\"Christopher Nolan\", rating=8.8)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3.2 TypedDict 提供了一种更简单的替代方案，使用 Python 内置的类型，适合在不需要运行时验证的情况下使用。\n",
    "\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "\n",
    "\n",
    "class MovieDict(TypedDict):\n",
    "  \"\"\"A movie with details.\"\"\"\n",
    "  title: Annotated[str, ..., \"The title of the movie\"]\n",
    "  year: Annotated[int, ..., \"The year the movie was released\"]\n",
    "  director: Annotated[str, ..., \"The director of the movie\"]\n",
    "  rating: Annotated[float, ..., \"The movie's rating out of 10\"]\n",
    "\n",
    "\n",
    "model_with_structure = model.with_structured_output(MovieDict)\n",
    "response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n",
    "print(response)  # {'title': 'Inception', 'year': 2010, 'director': 'Christopher Nolan', 'rating': 8.8}"
   ],
   "id": "6ccb94fa9b66d876",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3.3 为了获得最大的控制权或互操作性，您可以提供原始 JSON Schema。\n",
    "\n",
    "import json\n",
    "\n",
    "json_schema = {\n",
    "  \"title\": \"Movie\",\n",
    "  \"description\": \"A movie with details\",\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"title\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"The title of the movie\"\n",
    "    },\n",
    "    \"year\": {\n",
    "      \"type\": \"integer\",\n",
    "      \"description\": \"The year the movie was released\"\n",
    "    },\n",
    "    \"director\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"The director of the movie\"\n",
    "    },\n",
    "    \"rating\": {\n",
    "      \"type\": \"number\",\n",
    "      \"description\": \"The movie's rating out of 10\"\n",
    "    }\n",
    "  },\n",
    "  \"required\": [\"title\", \"year\", \"director\", \"rating\"]\n",
    "}\n",
    "\n",
    "model_with_structure = model.with_structured_output(\n",
    "  json_schema,\n",
    "  method=\"json_schema\",\n",
    ")\n",
    "response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n",
    "print(response)  # {'title': 'Inception', 'year': 2010, ...}"
   ],
   "id": "3029f270cb557999",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 嵌套结构\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Actor(BaseModel):\n",
    "  name: str\n",
    "  role: str\n",
    "\n",
    "\n",
    "class MovieDetails(BaseModel):\n",
    "  title: str\n",
    "  year: int\n",
    "  cast: list[Actor]\n",
    "  genres: list[str]\n",
    "  budget: float | None = Field(None, description=\"预算（单位：百万美元）\")\n",
    "\n",
    "\n",
    "model_with_structure = model.with_structured_output(MovieDetails)\n",
    "\n",
    "response = model_with_structure.invoke(\"使用中文提供电影《Inception》的信息\")\n",
    "print(response)"
   ],
   "id": "10dafa3cdbf168f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e973d7759f5af095",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "496d84ff",
   "metadata": {},
   "source": "## 4. 高级配置"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.1 多模态",
   "id": "4106270b887b7983"
  },
  {
   "cell_type": "code",
   "id": "8cba288d",
   "metadata": {},
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"openai:gpt-5\")\n",
    "\n",
    "response = model.invoke(\"Create a picture of a cat use base64\")\n",
    "print(response.content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f121690c",
   "metadata": {},
   "source": "### 4.2 推理"
  },
  {
   "cell_type": "code",
   "id": "1859c934",
   "metadata": {},
   "source": [
    "for chunk in model.stream(\"人生的意义是什么？\"):\n",
    "  print(chunk.content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = model.invoke(\"人生的意义是什么？\")\n",
    "reasoning_steps = [b for b in response.content_blocks if b[\"type\"] == \"reasoning\"]\n",
    "print(\" \".join(step[\"reasoning\"] for step in reasoning_steps))"
   ],
   "id": "e0af2a0b4c68bcc5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.3 Ollama 模型示例",
   "id": "c900ade113d0e583"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# 初始化本地模型\n",
    "model = ChatOllama(\n",
    "  model=\"deepseek-r1:8b\",  # 模型名称与 ollama list 中一致\n",
    "  temperature=0.7,  # 可选参数\n",
    ")\n",
    "\n",
    "# 调用模型\n",
    "response = model.invoke(\"请用中文解释 Transformer 的自注意力机制。\")\n",
    "print(response.content)\n"
   ],
   "id": "d58d0811b87370b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "85a76a96",
   "metadata": {},
   "source": [
    "## 4.3 高级配置 - 缓存"
   ]
  },
  {
   "cell_type": "code",
   "id": "c08da9d9",
   "metadata": {},
   "source": [
    "from langchain_core.caches import InMemoryCache\n",
    "from langchain_core.globals import set_llm_cache\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "response = model.invoke(\"Tell me a joke\")\n",
    "print(response)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = model.invoke(\"Tell me a joke\")  # Fast, from cache\n",
    "print(response)"
   ],
   "id": "672f0c99397bddf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.4 服务器端工具使用",
   "id": "ea1cab0a067010aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"openai:gpt-4o\")  ## 经测试 gpt-4o-tools 不提供 web_search 工具, 可能是代理服务器问题\n",
    "\n",
    "tool = {\"type\": \"web_search\"}\n",
    "model_with_tools = model.bind_tools([tool])\n",
    "\n",
    "response = model_with_tools.invoke(\"What was a positive news story from today?\")\n",
    "response.content_blocks"
   ],
   "id": "ff2ec0a9c9815a79",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "97425e5d",
   "metadata": {},
   "source": "### 4.5 高级配置 - 速率限制"
  },
  {
   "cell_type": "code",
   "id": "c325d658",
   "metadata": {},
   "source": [
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "\n",
    "rate_limiter = InMemoryRateLimiter(\n",
    "  requests_per_second=0.1,  # 表示每秒只能发 0.1 次请求，等价于 10 秒允许 1 次请求。\n",
    "  check_every_n_seconds=0.1,  # 限流器会每 0.1 秒（100 毫秒）检查一次，看看是不是可以放行请求\n",
    "  max_bucket_size=10,  # 采用令牌桶算法（token bucket）。允许一次性“积攒”最多 10 个请求作为突发流量。例如你空闲了 100 秒没请求，就能一次性发 10 个，但不会无限积累。\n",
    ")\n",
    "\n",
    "model = init_chat_model(\n",
    "  model=\"gpt-5\",\n",
    "  model_provider=\"openai\",\n",
    "  rate_limiter=rate_limiter\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.6 基础 URL 或代理",
   "id": "3ab14c5374a14324"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = init_chat_model(\n",
    "  model=\"MODEL_NAME\",\n",
    "  model_provider=\"openai\",\n",
    "  base_url=\"BASE_URL\",\n",
    "  api_key=\"YOUR_API_KEY\",\n",
    ")"
   ],
   "id": "1167b981566031d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "  model=\"gpt-4o\",\n",
    "  openai_proxy=\"http://proxy.example.com:8080\",\n",
    "  api_key=\"sk_xxx\",\n",
    ")"
   ],
   "id": "5161de4e12191690",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.7 日志概率\n",
    "\n",
    "某些模型可以通过在初始化模型时设置 **logprobs** 参数，来配置返回 **token 级别的对数概率（log probabilities）**，用于表示每个给定 token 的可能性。\n"
   ],
   "id": "c61487d622a5ee76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = init_chat_model(\n",
    "  model=\"gpt-4o\",\n",
    "  model_provider=\"openai\"\n",
    ").bind(logprobs=True)\n",
    "\n",
    "response = model.invoke(\"人生的意义是什么？\")\n",
    "print(response.response_metadata[\"logprobs\"])"
   ],
   "id": "5ff178ee162d4985",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fdc58de3",
   "metadata": {},
   "source": "### 4.8 高级配置 - 令牌使用"
  },
  {
   "cell_type": "code",
   "id": "7a37699c",
   "metadata": {},
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.callbacks import UsageMetadataCallbackHandler\n",
    "\n",
    "model_1 = init_chat_model(model=\"openai:gpt-4o-mini\")\n",
    "model_2 = init_chat_model(model=\"openai:gpt-5\")\n",
    "\n",
    "callback = UsageMetadataCallbackHandler()\n",
    "result_1 = model_1.invoke(\"Hello\", config={\"callbacks\": [callback]})\n",
    "result_2 = model_2.invoke(\"Hello\", config={\"callbacks\": [callback]})\n",
    "callback.usage_metadata"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.callbacks import get_usage_metadata_callback\n",
    "\n",
    "model_1 = init_chat_model(model=\"openai:gpt-4o-mini\")\n",
    "model_2 = init_chat_model(model=\"openai:gpt-5\")\n",
    "\n",
    "with get_usage_metadata_callback() as cb:\n",
    "  model_1.invoke(\"Hello\")\n",
    "  model_2.invoke(\"Hello\")\n",
    "  print(cb.usage_metadata)"
   ],
   "id": "7d9daa9245fb69c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0790b5d4",
   "metadata": {},
   "source": "### 4.9 - 调用配置"
  },
  {
   "cell_type": "code",
   "id": "bdfd4f04",
   "metadata": {},
   "source": [
    "response = model.invoke(\n",
    "  \"你好，讲个笑话吧！\",\n",
    "  config={\n",
    "    \"run_name\": \"joke_generation\",  # Custom name for this run\n",
    "    \"tags\": [\"humor\", \"demo\"],  # Tags for categorization\n",
    "    \"metadata\": {\"user_id\": \"123\"},  # Custom metadata\n",
    "  }\n",
    ")\n",
    "\n",
    "print(response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7f606664",
   "metadata": {},
   "source": "### 4.10 高级配置 - 可配置模型"
  },
  {
   "cell_type": "code",
   "id": "d5b28267",
   "metadata": {},
   "source": [
    "# 创建一个可配置的模型实例\n",
    "# 配置字段包括：model, model_provider, temperature, max_tokens\n",
    "\n",
    "first_model = init_chat_model(\n",
    "  model=\"gpt-4.1-mini\",\n",
    "  temperature=0,\n",
    "  configurable_fields=(\"model\", \"model_provider\", \"temperature\", \"max_tokens\"),\n",
    "  config_prefix=\"first\",  # Useful when you have a chain with multiple models\n",
    ")\n",
    "\n",
    "# 使用默认配置调用模型\n",
    "first_model.invoke(\"what's your name\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 在运行时覆盖配置字段\n",
    "response = first_model.invoke(\n",
    "  \"Tell me a joke\",\n",
    "  config={\n",
    "    \"model_provider\": \"openai\",\n",
    "    \"model\": \"gpt-5\",\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 1000,\n",
    "  }\n",
    ")"
   ],
   "id": "4f80fb6f6d9bfe18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 以声明方式使用可配置模型\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class GetWeather(BaseModel):\n",
    "  \"\"\"Get the current weather in a given location\"\"\"\n",
    "  location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
    "\n",
    "\n",
    "class GetPopulation(BaseModel):\n",
    "  \"\"\"Get the current population in a given location\"\"\"\n",
    "  location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
    "\n",
    "# init_chat_model() 不直接实例化具体模型，而是创建了一个“声明式可配置模型”，\n",
    "# 你可以在调用时通过 config 参数动态指定模型的配置。\n",
    "model = init_chat_model(temperature=0)\n",
    "model_with_tools = model.bind_tools([GetWeather, GetPopulation])\n",
    "\n",
    "model_with_tools.invoke(\n",
    "  \"what's bigger in 2024 LA or NYC\", config={\"configurable\": {\"model\": \"gpt-4.1-mini\"}}\n",
    ").tool_calls"
   ],
   "id": "e3cdb0941bbe17f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_with_tools.invoke(\n",
    "    \"what's bigger in 2024 LA or NYC\",\n",
    "        config={\"configurable\": {\"model\": \"gpt-5\"}},\n",
    ").tool_calls"
   ],
   "id": "d8120b9caf38e510",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
