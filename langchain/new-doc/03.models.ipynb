{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bc583aebb7345ce",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "id": "d74cf04036e0f2ce",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# 1. langchain çš„ init_chat_model å‡½æ•°\n",
    "# model_provider æ˜¯ LangChain åœ¨ v1.0 ä¹‹åä¸ºäº†â€œè§£è€¦æ¨¡å‹åç§°ä¸ä¾›åº”å•†â€è€Œå¼•å…¥çš„\n",
    "model = init_chat_model(model_provider=\"openai\", model=\"gpt-3.5-turbo\")  # \n",
    "\n",
    "response = model.invoke(\"ä¸ºä»€ä¹ˆé¹¦é¹‰ä¼šè¯´è¯ï¼Ÿ\")\n",
    "\n",
    "print(response)\n",
    "print(response.content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ba51eb46",
   "metadata": {},
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 2. langchain_openai çš„ ChatOpenAI ç±»\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "response = model.invoke(\"å¦‚ä½•å­¦ä¹  LangChain\")\n",
    "\n",
    "print(response)\n",
    "\n",
    "print(response.content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cdadf60c9f70ad62",
   "metadata": {},
   "source": [
    "## æ¨¡å‹ä¼ å…¥çš„å‚æ•°\n",
    "\n",
    "- model\n",
    "- api_key\n",
    "- temperature\n",
    "- max_tokens\n",
    "- max-retries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925270bef16e4aa5",
   "metadata": {},
   "source": [
    "## 1. è°ƒç”¨"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\n",
    "  \"openai:gpt-3.5-turbo\",\n",
    "  # Kwargs passed to the model:\n",
    "  temperature=0.7,\n",
    "  timeout=30,\n",
    "  max_tokens=1000,\n",
    ")\n",
    "response = model.invoke(\"ä¸ºä»€ä¹ˆé¹¦é¹‰çš„ç¾½æ¯›é¢œè‰²é²œè‰³ï¼Ÿ\")\n",
    "print(response.content)\n"
   ],
   "id": "4119eff47fc01494",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# å­—å…¸æ ¼å¼\n",
    "from langchain.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "conversation = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates Chinese to English.\"},\n",
    "  {\"role\": \"user\", \"content\": \"ç¿»è¯‘ï¼šæˆ‘çƒ­çˆ±ç¼–ç¨‹\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"I love programming.\"},\n",
    "  {\"role\": \"user\", \"content\": \"ç¿»è¯‘ï¼šæˆ‘å–œæ¬¢æ„å»ºåº”ç”¨ç¨‹åº\"}\n",
    "]\n",
    "\n",
    "response = model.invoke(conversation)\n",
    "print(response)  # AIMessage(\"I enjoy building applications.\")"
   ],
   "id": "6aa409beb768d3b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# æ¶ˆæ¯æ ¼å¼\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "conversation = [\n",
    "  SystemMessage(content=\"You are a helpful assistant that translates Chinese to English.\"),\n",
    "  HumanMessage(content=\"ç¿»è¯‘ï¼šæˆ‘çƒ­çˆ±ç¼–ç¨‹\"),\n",
    "  AIMessage(content=\"I love programming.\"),\n",
    "  HumanMessage(content=\"ç¿»è¯‘ï¼šæˆ‘å–œæ¬¢æ„å»ºåº”ç”¨ç¨‹åº\")\n",
    "]\n",
    "\n",
    "response = model.invoke(conversation)\n",
    "print(response)  # AIMessage(\"I enjoy building applications.\")"
   ],
   "id": "6905199a7d6822d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### æµå¼ä¼ è¾“",
   "id": "9e4c3e10450ff9d6"
  },
  {
   "cell_type": "code",
   "id": "5c61bad4ba517cca",
   "metadata": {},
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\n",
    "  \"openai:gpt-3.5-turbo\",\n",
    "  streaming=True,\n",
    ")\n",
    "\n",
    "for chunk in model.stream(\"ä¸ºä»€ä¹ˆé¹¦é¹‰çš„ç¾½æ¯›é¢œè‰²é²œè‰³ï¼Ÿ\"):\n",
    "  print(chunk.text, end=\"\", flush=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7b8965f8f60d0d9b",
   "metadata": {},
   "source": [
    "full = None  # None | AIMessageChunk\n",
    "for chunk in model.stream(\"What color is the sky?\"):\n",
    "  full = chunk if full is None else full + chunk\n",
    "  print(full.text)\n",
    "\n",
    "# The\n",
    "# The sky\n",
    "# The sky is\n",
    "# The sky is typically\n",
    "# The sky is typically blue\n",
    "# ...\n",
    "\n",
    "print(full.content)\n",
    "# Usually blue during the day. That blue comes from Rayleigh scattering: sunlight hits the atmosphere and the shorter blue wavelengths scatter in all directions. The color can change: red/orange/pink at sunrise and sunset, gray on cloudy days, and dark at night (with stars visible). Want the answer for a specific time or place?"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "  \"\"\"Get the weather for a city.\"\"\"\n",
    "  return f\"The weather in {city} is sunny.\"\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "agent = create_agent(\n",
    "  model=model,\n",
    "  tools=[get_weather],\n",
    ")\n",
    "\n",
    "# ğŸŒŠ å®æ—¶æµå¼è¾“å‡º\n",
    "for chunk in agent.stream(\n",
    "  {\"messages\": [{\"role\": \"user\", \"content\": \"æŸ¥ä¸‹æ·±åœ³å¤©æ°”ï¼Œç„¶åå†™é¦–è¯—\"}]},\n",
    "  stream_mode=\"values\",  # è¾“å‡º AgentState ä¸­çš„æ¯ä¸ªé˜¶æ®µ\n",
    "):\n",
    "  latest_msg = chunk[\"messages\"][-1]\n",
    "  if latest_msg.content:\n",
    "    print(latest_msg.content, end=\"\", flush=True)\n"
   ],
   "id": "3474a0d3e21d5981",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8608d0b4d4126253",
   "metadata": {},
   "source": [
    "### æ‰¹é‡ Batch"
   ]
  },
  {
   "cell_type": "code",
   "id": "f24ff03cf16772e5",
   "metadata": {},
   "source": [
    "# è¿”å›æ•´ä¸ªæ‰¹æ¬¡çš„æœ€ç»ˆè¾“å‡º\n",
    "responses = model.batch([\n",
    "  \"Why do parrots have colorful feathers?\",\n",
    "  \"How do airplanes fly?\",\n",
    "  \"What is quantum computing?\"\n",
    "])\n",
    "for response in responses:\n",
    "  print(response.content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4cb3a35a7d81904f",
   "metadata": {},
   "source": [
    "# é€ä¸ªæ¥æ”¶æ¯ä¸ªè¾“å…¥ç”Ÿæˆå®Œæˆæ—¶çš„è¾“å‡º.åœ¨ä½¿ç”¨ batch_as_completed() æ—¶ï¼Œç»“æœå¯èƒ½ä¼šä¹±åºåˆ°è¾¾ã€‚æ¯ä¸ªç»“æœéƒ½åŒ…å«è¾“å…¥ç´¢å¼•ï¼Œä»¥ä¾¿åœ¨éœ€è¦æ—¶åŒ¹é…ä»¥é‡å»ºåŸå§‹é¡ºåºã€‚\n",
    "for response in model.batch_as_completed([\n",
    "  \"ä¸ºä»€ä¹ˆé¹¦é¹‰ä¼šè¯´è¯ï¼Ÿ\",\n",
    "  \"å¦‚ä½•é£æœºé£è¡Œï¼Ÿ\",\n",
    "  \"ä»€ä¹ˆæ˜¯é‡å­è®¡ç®—ï¼Ÿ\"\n",
    "], config={\"batch_size\": 1, \"max_concurrency\": 2}):  # æ‰¹é‡å¤§å°ä¸º1ï¼Œå¹¶å‘æ•°ä¸º2\n",
    "\n",
    "  print(response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a6d175abec11c3f2",
   "metadata": {},
   "source": [
    "## 2.å·¥å…·è°ƒç”¨"
   ]
  },
  {
   "cell_type": "code",
   "id": "d214a2d8c650fcf3",
   "metadata": {},
   "source": [
    "from langchain.tools import tool\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "  \"\"\"Get the weather at a location.\"\"\"\n",
    "  return f\"It's sunny in {location}.\"\n",
    "\n",
    "\n",
    "model = init_chat_model(\n",
    "  \"openai:gpt-3.5-turbo\",\n",
    "  streaming=True,\n",
    ")\n",
    "\n",
    "model_with_tools = model.bind_tools([get_weather])\n",
    "\n",
    "response = model_with_tools.invoke(\"What's the weather like in Boston?\")\n",
    "for tool_call in response.tool_calls:\n",
    "  # View tool calls made by the model\n",
    "  print(f\"Tool: {tool_call['name']}\")\n",
    "  print(f\"Args: {tool_call['args']}\")\n",
    "\n",
    "response.pretty_print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e9324cc61d64bf2f",
   "metadata": {},
   "source": [
    "## 2.1 å·¥å…·æ‰§è¡Œå¾ªç¯"
   ]
  },
  {
   "cell_type": "code",
   "id": "2f97b4cb81de3d7d",
   "metadata": {},
   "source": [
    "# å°†ï¼ˆå¯èƒ½å¤šä¸ªï¼‰å·¥å…·ç»‘å®šåˆ°æ¨¡å‹\n",
    "model_with_tools = model.bind_tools([get_weather])\n",
    "\n",
    "# Step 1: æ¨¡å‹ç”Ÿæˆå·¥å…·è°ƒç”¨\n",
    "messages = [{\"role\": \"user\", \"content\": \"ä»Šå¤©æ³¢å£«é¡¿çš„å¤©æ°”å¦‚ä½•ï¼Ÿ\"}]\n",
    "ai_msg = model_with_tools.invoke(messages)\n",
    "messages.append(ai_msg)\n",
    "\n",
    "print(messages)\n",
    "\n",
    "# Step 2: æ‰§è¡Œå·¥å…·å¹¶æ”¶é›†ç»“æœ\n",
    "for tool_call in ai_msg.tool_calls:\n",
    "  # ä½¿ç”¨ç”Ÿæˆçš„å‚æ•°æ‰§è¡Œè¯¥å·¥å…·\n",
    "  tool_result = get_weather.invoke(tool_call)\n",
    "  messages.append(tool_result)\n",
    "\n",
    "print(messages)\n",
    "\n",
    "# Step 3: å°†ç»“æœä¼ å›æ¨¡å‹ä»¥è·å¾—æœ€ç»ˆå“åº”\n",
    "final_response = model_with_tools.invoke(messages)\n",
    "print(final_response.text)\n",
    "# \"The current weather in Boston is 72Â°F and sunny.\"\n",
    "\n",
    "final_response.pretty_print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3481e766e57a6d8e",
   "metadata": {},
   "source": [
    "2.2 å¼ºåˆ¶å·¥å…·è°ƒç”¨\n",
    "\n",
    "å¼ºåˆ¶ä½¿ç”¨ä»»ä½•å·¥å…·ï¼š \n",
    "```python\n",
    "model_with_tools = model.bind_tools([tool_1], tool_choice=\"any\")\n",
    "```\n",
    "\n",
    "å¼ºåˆ¶ä½¿ç”¨ç‰¹å®šå·¥å…·\n",
    "```python\n",
    "model_with_tools = model.bind_tools([tool_1], tool_choice=\"tool_1\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b8b67a8c5ef9e4",
   "metadata": {},
   "source": [
    "### 2.3 å¹¶è¡Œå·¥å…·è°ƒç”¨\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b271bd0c6e16f768",
   "metadata": {},
   "source": [
    "model_with_tools = model.bind_tools([get_weather])\n",
    "\n",
    "response = model_with_tools.invoke(\n",
    "  \"What's the weather in Boston and Tokyo?\"\n",
    ")\n",
    "\n",
    "# The model may generate multiple tool calls\n",
    "print(response.tool_calls)\n",
    "# [\n",
    "#   {'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': 'call_1'},\n",
    "#   {'name': 'get_weather', 'args': {'location': 'Tokyo'}, 'id': 'call_2'}\n",
    "# ]\n",
    "\n",
    "\n",
    "# Execute all tools (can be done in parallel with async)\n",
    "results = []\n",
    "for tool_call in response.tool_calls:\n",
    "  if tool_call['name'] == 'get_weather':\n",
    "    result = get_weather.invoke(tool_call)\n",
    "  # ...\n",
    "  results.append(result)\n",
    "\n",
    "print(results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "å¤§å¤šæ•°æ”¯æŒå·¥å…·è°ƒç”¨çš„æ¨¡å‹é»˜è®¤å¯ç”¨å¹¶è¡Œå·¥å…·è°ƒç”¨ã€‚ä¸€äº›æ¨¡å‹ï¼ˆåŒ…æ‹¬ OpenAI å’Œ Anthropicï¼‰å…è®¸æ‚¨ç¦ç”¨æ­¤åŠŸèƒ½ã€‚è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œè¯·è®¾ç½® parallel_tool_calls=False\n",
    "```python\n",
    "model.bind_tools([get_weather], parallel_tool_calls=False)\n",
    "```\n"
   ],
   "id": "4e9da20fe98cb10b"
  },
  {
   "cell_type": "markdown",
   "id": "3f104d5b64ab3cd7",
   "metadata": {},
   "source": [
    "### 2.4 æµå¼å·¥å…·è°ƒç”¨"
   ]
  },
  {
   "cell_type": "code",
   "id": "5147143df3209fe7",
   "metadata": {},
   "source": [
    "for chunk in model_with_tools.stream(\n",
    "  \"What's the weather in Boston and Tokyo?\"\n",
    "):\n",
    "  # Tool call chunks arrive progressively\n",
    "  for tool_chunk in chunk.tool_call_chunks:\n",
    "    if name := tool_chunk.get(\"name\"):\n",
    "      print(f\"Tool: {name}\")\n",
    "    if id_ := tool_chunk.get(\"id\"):\n",
    "      print(f\"ID: {id_}\")\n",
    "    if args := tool_chunk.get(\"args\"):\n",
    "      print(f\"Args: {args}\")\n",
    "\n",
    "# Output:\n",
    "# Tool: get_weather\n",
    "# ID: call_SvMlU1TVIZugrFLckFE2ceRE\n",
    "# Args: {\"lo\n",
    "# Args: catio\n",
    "# Args: n\": \"B\n",
    "# Args: osto\n",
    "# Args: n\"}\n",
    "# Tool: get_weather\n",
    "# ID: call_QMZdy6qInx13oWKE7KhuhOLR\n",
    "# Args: {\"lo\n",
    "# Args: catio\n",
    "# Args: n\": \"T\n",
    "# Args: okyo\n",
    "# Args: \"}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e469304569800b53",
   "metadata": {},
   "source": [
    "gathered = None\n",
    "for chunk in model_with_tools.stream(\"What's the weather in Boston?\"):\n",
    "  gathered = chunk if gathered is None else gathered + chunk\n",
    "  print(gathered)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a1f4cd81",
   "metadata": {},
   "source": [
    "## 3.ç»“æ„æ ‘è¾“å‡º"
   ]
  },
  {
   "cell_type": "code",
   "id": "4599cba9",
   "metadata": {},
   "source": [
    "# 3.1 Pydantic æ¨¡å‹ æä¾›äº†æœ€ä¸°å¯Œçš„åŠŸèƒ½é›†ï¼ŒåŒ…æ‹¬å­—æ®µéªŒè¯ã€æè¿°å’ŒåµŒå¥—ç»“æ„ã€‚\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "class Movie(BaseModel):\n",
    "  \"\"\"A movie with details.\"\"\"\n",
    "  title: str = Field(..., description=\"The title of the movie\")\n",
    "  year: int = Field(..., description=\"The year the movie was released\")\n",
    "  director: str = Field(..., description=\"The director of the movie\")\n",
    "  rating: float = Field(..., description=\"The movie's rating out of 10\")\n",
    "\n",
    "\n",
    "model = init_chat_model(\"openai:gpt-5\")\n",
    "\n",
    "# include_raw=True ä¼šåœ¨å“åº”ä¸­åŒ…å«åŸå§‹æ–‡æœ¬ï¼Œä¾¿äºè°ƒè¯•å’Œè®°å½•ã€‚\n",
    "model_with_structure = model.with_structured_output(Movie, include_raw=True)\n",
    "\n",
    "response = model_with_structure.invoke(\"æä¾›ç”µå½±ã€ŠInceptionã€‹çš„ä¿¡æ¯ã€‚\")\n",
    "\n",
    "print(response)  # Movie(title=\"Inception\", year=2010, director=\"Christopher Nolan\", rating=8.8)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3.2 TypedDict æä¾›äº†ä¸€ç§æ›´ç®€å•çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½¿ç”¨ Python å†…ç½®çš„ç±»å‹ï¼Œé€‚åˆåœ¨ä¸éœ€è¦è¿è¡Œæ—¶éªŒè¯çš„æƒ…å†µä¸‹ä½¿ç”¨ã€‚\n",
    "\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "\n",
    "\n",
    "class MovieDict(TypedDict):\n",
    "  \"\"\"A movie with details.\"\"\"\n",
    "  title: Annotated[str, ..., \"The title of the movie\"]\n",
    "  year: Annotated[int, ..., \"The year the movie was released\"]\n",
    "  director: Annotated[str, ..., \"The director of the movie\"]\n",
    "  rating: Annotated[float, ..., \"The movie's rating out of 10\"]\n",
    "\n",
    "\n",
    "model_with_structure = model.with_structured_output(MovieDict)\n",
    "response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n",
    "print(response)  # {'title': 'Inception', 'year': 2010, 'director': 'Christopher Nolan', 'rating': 8.8}"
   ],
   "id": "6ccb94fa9b66d876",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3.3 ä¸ºäº†è·å¾—æœ€å¤§çš„æ§åˆ¶æƒæˆ–äº’æ“ä½œæ€§ï¼Œæ‚¨å¯ä»¥æä¾›åŸå§‹ JSON Schemaã€‚\n",
    "\n",
    "import json\n",
    "\n",
    "json_schema = {\n",
    "  \"title\": \"Movie\",\n",
    "  \"description\": \"A movie with details\",\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"title\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"The title of the movie\"\n",
    "    },\n",
    "    \"year\": {\n",
    "      \"type\": \"integer\",\n",
    "      \"description\": \"The year the movie was released\"\n",
    "    },\n",
    "    \"director\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"The director of the movie\"\n",
    "    },\n",
    "    \"rating\": {\n",
    "      \"type\": \"number\",\n",
    "      \"description\": \"The movie's rating out of 10\"\n",
    "    }\n",
    "  },\n",
    "  \"required\": [\"title\", \"year\", \"director\", \"rating\"]\n",
    "}\n",
    "\n",
    "model_with_structure = model.with_structured_output(\n",
    "  json_schema,\n",
    "  method=\"json_schema\",\n",
    ")\n",
    "response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n",
    "print(response)  # {'title': 'Inception', 'year': 2010, ...}"
   ],
   "id": "3029f270cb557999",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# åµŒå¥—ç»“æ„\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Actor(BaseModel):\n",
    "  name: str\n",
    "  role: str\n",
    "\n",
    "\n",
    "class MovieDetails(BaseModel):\n",
    "  title: str\n",
    "  year: int\n",
    "  cast: list[Actor]\n",
    "  genres: list[str]\n",
    "  budget: float | None = Field(None, description=\"é¢„ç®—ï¼ˆå•ä½ï¼šç™¾ä¸‡ç¾å…ƒï¼‰\")\n",
    "\n",
    "\n",
    "model_with_structure = model.with_structured_output(MovieDetails)\n",
    "\n",
    "response = model_with_structure.invoke(\"ä½¿ç”¨ä¸­æ–‡æä¾›ç”µå½±ã€ŠInceptionã€‹çš„ä¿¡æ¯\")\n",
    "print(response)"
   ],
   "id": "10dafa3cdbf168f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e973d7759f5af095",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "496d84ff",
   "metadata": {},
   "source": "## 4. é«˜çº§é…ç½®"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.1 å¤šæ¨¡æ€",
   "id": "4106270b887b7983"
  },
  {
   "cell_type": "code",
   "id": "8cba288d",
   "metadata": {},
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"openai:gpt-5\")\n",
    "\n",
    "response = model.invoke(\"Create a picture of a cat use base64\")\n",
    "print(response.content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f121690c",
   "metadata": {},
   "source": "### 4.2 æ¨ç†"
  },
  {
   "cell_type": "code",
   "id": "1859c934",
   "metadata": {},
   "source": [
    "for chunk in model.stream(\"äººç”Ÿçš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ\"):\n",
    "  print(chunk.content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = model.invoke(\"äººç”Ÿçš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ\")\n",
    "reasoning_steps = [b for b in response.content_blocks if b[\"type\"] == \"reasoning\"]\n",
    "print(\" \".join(step[\"reasoning\"] for step in reasoning_steps))"
   ],
   "id": "e0af2a0b4c68bcc5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.3 Ollama æ¨¡å‹ç¤ºä¾‹",
   "id": "c900ade113d0e583"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹\n",
    "model = ChatOllama(\n",
    "  model=\"deepseek-r1:8b\",  # æ¨¡å‹åç§°ä¸ ollama list ä¸­ä¸€è‡´\n",
    "  temperature=0.7,  # å¯é€‰å‚æ•°\n",
    ")\n",
    "\n",
    "# è°ƒç”¨æ¨¡å‹\n",
    "response = model.invoke(\"è¯·ç”¨ä¸­æ–‡è§£é‡Š Transformer çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚\")\n",
    "print(response.content)\n"
   ],
   "id": "d58d0811b87370b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "85a76a96",
   "metadata": {},
   "source": [
    "## 4.3 é«˜çº§é…ç½® - ç¼“å­˜"
   ]
  },
  {
   "cell_type": "code",
   "id": "c08da9d9",
   "metadata": {},
   "source": [
    "from langchain_core.caches import InMemoryCache\n",
    "from langchain_core.globals import set_llm_cache\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "response = model.invoke(\"Tell me a joke\")\n",
    "print(response)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = model.invoke(\"Tell me a joke\")  # Fast, from cache\n",
    "print(response)"
   ],
   "id": "672f0c99397bddf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.4 æœåŠ¡å™¨ç«¯å·¥å…·ä½¿ç”¨",
   "id": "ea1cab0a067010aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"openai:gpt-4o\")  ## ç»æµ‹è¯• gpt-4o-tools ä¸æä¾› web_search å·¥å…·, å¯èƒ½æ˜¯ä»£ç†æœåŠ¡å™¨é—®é¢˜\n",
    "\n",
    "tool = {\"type\": \"web_search\"}\n",
    "model_with_tools = model.bind_tools([tool])\n",
    "\n",
    "response = model_with_tools.invoke(\"What was a positive news story from today?\")\n",
    "response.content_blocks"
   ],
   "id": "ff2ec0a9c9815a79",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "97425e5d",
   "metadata": {},
   "source": "### 4.5 é«˜çº§é…ç½® - é€Ÿç‡é™åˆ¶"
  },
  {
   "cell_type": "code",
   "id": "c325d658",
   "metadata": {},
   "source": [
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "\n",
    "rate_limiter = InMemoryRateLimiter(\n",
    "  requests_per_second=0.1,  # è¡¨ç¤ºæ¯ç§’åªèƒ½å‘ 0.1 æ¬¡è¯·æ±‚ï¼Œç­‰ä»·äº 10 ç§’å…è®¸ 1 æ¬¡è¯·æ±‚ã€‚\n",
    "  check_every_n_seconds=0.1,  # é™æµå™¨ä¼šæ¯ 0.1 ç§’ï¼ˆ100 æ¯«ç§’ï¼‰æ£€æŸ¥ä¸€æ¬¡ï¼Œçœ‹çœ‹æ˜¯ä¸æ˜¯å¯ä»¥æ”¾è¡Œè¯·æ±‚\n",
    "  max_bucket_size=10,  # é‡‡ç”¨ä»¤ç‰Œæ¡¶ç®—æ³•ï¼ˆtoken bucketï¼‰ã€‚å…è®¸ä¸€æ¬¡æ€§â€œç§¯æ”’â€æœ€å¤š 10 ä¸ªè¯·æ±‚ä½œä¸ºçªå‘æµé‡ã€‚ä¾‹å¦‚ä½ ç©ºé—²äº† 100 ç§’æ²¡è¯·æ±‚ï¼Œå°±èƒ½ä¸€æ¬¡æ€§å‘ 10 ä¸ªï¼Œä½†ä¸ä¼šæ— é™ç§¯ç´¯ã€‚\n",
    ")\n",
    "\n",
    "model = init_chat_model(\n",
    "  model=\"gpt-5\",\n",
    "  model_provider=\"openai\",\n",
    "  rate_limiter=rate_limiter\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.6 åŸºç¡€ URL æˆ–ä»£ç†",
   "id": "3ab14c5374a14324"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = init_chat_model(\n",
    "  model=\"MODEL_NAME\",\n",
    "  model_provider=\"openai\",\n",
    "  base_url=\"BASE_URL\",\n",
    "  api_key=\"YOUR_API_KEY\",\n",
    ")"
   ],
   "id": "1167b981566031d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "  model=\"gpt-4o\",\n",
    "  openai_proxy=\"http://proxy.example.com:8080\",\n",
    "  api_key=\"sk_xxx\",\n",
    ")"
   ],
   "id": "5161de4e12191690",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.7 æ—¥å¿—æ¦‚ç‡\n",
    "\n",
    "æŸäº›æ¨¡å‹å¯ä»¥é€šè¿‡åœ¨åˆå§‹åŒ–æ¨¡å‹æ—¶è®¾ç½® **logprobs** å‚æ•°ï¼Œæ¥é…ç½®è¿”å› **token çº§åˆ«çš„å¯¹æ•°æ¦‚ç‡ï¼ˆlog probabilitiesï¼‰**ï¼Œç”¨äºè¡¨ç¤ºæ¯ä¸ªç»™å®š token çš„å¯èƒ½æ€§ã€‚\n"
   ],
   "id": "c61487d622a5ee76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = init_chat_model(\n",
    "  model=\"gpt-4o\",\n",
    "  model_provider=\"openai\"\n",
    ").bind(logprobs=True)\n",
    "\n",
    "response = model.invoke(\"äººç”Ÿçš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ\")\n",
    "print(response.response_metadata[\"logprobs\"])"
   ],
   "id": "5ff178ee162d4985",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fdc58de3",
   "metadata": {},
   "source": "### 4.8 é«˜çº§é…ç½® - ä»¤ç‰Œä½¿ç”¨"
  },
  {
   "cell_type": "code",
   "id": "7a37699c",
   "metadata": {},
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.callbacks import UsageMetadataCallbackHandler\n",
    "\n",
    "model_1 = init_chat_model(model=\"openai:gpt-4o-mini\")\n",
    "model_2 = init_chat_model(model=\"openai:gpt-5\")\n",
    "\n",
    "callback = UsageMetadataCallbackHandler()\n",
    "result_1 = model_1.invoke(\"Hello\", config={\"callbacks\": [callback]})\n",
    "result_2 = model_2.invoke(\"Hello\", config={\"callbacks\": [callback]})\n",
    "callback.usage_metadata"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.callbacks import get_usage_metadata_callback\n",
    "\n",
    "model_1 = init_chat_model(model=\"openai:gpt-4o-mini\")\n",
    "model_2 = init_chat_model(model=\"openai:gpt-5\")\n",
    "\n",
    "with get_usage_metadata_callback() as cb:\n",
    "  model_1.invoke(\"Hello\")\n",
    "  model_2.invoke(\"Hello\")\n",
    "  print(cb.usage_metadata)"
   ],
   "id": "7d9daa9245fb69c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0790b5d4",
   "metadata": {},
   "source": "### 4.9 - è°ƒç”¨é…ç½®"
  },
  {
   "cell_type": "code",
   "id": "bdfd4f04",
   "metadata": {},
   "source": [
    "response = model.invoke(\n",
    "  \"ä½ å¥½ï¼Œè®²ä¸ªç¬‘è¯å§ï¼\",\n",
    "  config={\n",
    "    \"run_name\": \"joke_generation\",  # Custom name for this run\n",
    "    \"tags\": [\"humor\", \"demo\"],  # Tags for categorization\n",
    "    \"metadata\": {\"user_id\": \"123\"},  # Custom metadata\n",
    "  }\n",
    ")\n",
    "\n",
    "print(response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7f606664",
   "metadata": {},
   "source": "### 4.10 é«˜çº§é…ç½® - å¯é…ç½®æ¨¡å‹"
  },
  {
   "cell_type": "code",
   "id": "d5b28267",
   "metadata": {},
   "source": [
    "# åˆ›å»ºä¸€ä¸ªå¯é…ç½®çš„æ¨¡å‹å®ä¾‹\n",
    "# é…ç½®å­—æ®µåŒ…æ‹¬ï¼šmodel, model_provider, temperature, max_tokens\n",
    "\n",
    "first_model = init_chat_model(\n",
    "  model=\"gpt-4.1-mini\",\n",
    "  temperature=0,\n",
    "  configurable_fields=(\"model\", \"model_provider\", \"temperature\", \"max_tokens\"),\n",
    "  config_prefix=\"first\",  # Useful when you have a chain with multiple models\n",
    ")\n",
    "\n",
    "# ä½¿ç”¨é»˜è®¤é…ç½®è°ƒç”¨æ¨¡å‹\n",
    "first_model.invoke(\"what's your name\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# åœ¨è¿è¡Œæ—¶è¦†ç›–é…ç½®å­—æ®µ\n",
    "response = first_model.invoke(\n",
    "  \"Tell me a joke\",\n",
    "  config={\n",
    "    \"model_provider\": \"openai\",\n",
    "    \"model\": \"gpt-5\",\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 1000,\n",
    "  }\n",
    ")"
   ],
   "id": "4f80fb6f6d9bfe18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ä»¥å£°æ˜æ–¹å¼ä½¿ç”¨å¯é…ç½®æ¨¡å‹\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class GetWeather(BaseModel):\n",
    "  \"\"\"Get the current weather in a given location\"\"\"\n",
    "  location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
    "\n",
    "\n",
    "class GetPopulation(BaseModel):\n",
    "  \"\"\"Get the current population in a given location\"\"\"\n",
    "  location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
    "\n",
    "# init_chat_model() ä¸ç›´æ¥å®ä¾‹åŒ–å…·ä½“æ¨¡å‹ï¼Œè€Œæ˜¯åˆ›å»ºäº†ä¸€ä¸ªâ€œå£°æ˜å¼å¯é…ç½®æ¨¡å‹â€ï¼Œ\n",
    "# ä½ å¯ä»¥åœ¨è°ƒç”¨æ—¶é€šè¿‡ config å‚æ•°åŠ¨æ€æŒ‡å®šæ¨¡å‹çš„é…ç½®ã€‚\n",
    "model = init_chat_model(temperature=0)\n",
    "model_with_tools = model.bind_tools([GetWeather, GetPopulation])\n",
    "\n",
    "model_with_tools.invoke(\n",
    "  \"what's bigger in 2024 LA or NYC\", config={\"configurable\": {\"model\": \"gpt-4.1-mini\"}}\n",
    ").tool_calls"
   ],
   "id": "e3cdb0941bbe17f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_with_tools.invoke(\n",
    "    \"what's bigger in 2024 LA or NYC\",\n",
    "        config={\"configurable\": {\"model\": \"gpt-5\"}},\n",
    ").tool_calls"
   ],
   "id": "d8120b9caf38e510",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
