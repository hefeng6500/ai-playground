"""
RAG (Retrieval-Augmented Generation) åº”ç”¨
==========================================

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ RAG åº”ç”¨ï¼Œæ”¯æŒï¼š
- æ–‡æ¡£åŠ è½½å’Œé¢„å¤„ç†
- å‘é‡å­˜å‚¨å’Œæ£€ç´¢
- åŸºäºæ£€ç´¢çš„ç”Ÿæˆ
- äº¤äº’å¼é—®ç­”

ä½¿ç”¨æ–¹æ³•ï¼š
1. å®‰è£…ä¾èµ–ï¼špip install -r requirements.txt
2. è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¦‚ OPENAI_API_KEY æˆ–å…¶ä»–æ¨¡å‹ API keyï¼‰
3. è¿è¡Œï¼špython main.py
"""

import os
import getpass
from typing import List, Optional
from pathlib import Path

from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import (
    TextLoader,
    PyPDFLoader,
    DirectoryLoader,
    UnstructuredMarkdownLoader,
)


class RAGApplication:
    """RAG åº”ç”¨ä¸»ç±»"""

    def __init__(
        self,
        embeddings_model=None,
        llm_model=None,
        vector_store_path: str = "./vector_store",
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
    ):
        """
        åˆå§‹åŒ– RAG åº”ç”¨

        Args:
            embeddings_model: åµŒå…¥æ¨¡å‹å®ä¾‹
            llm_model: LLM æ¨¡å‹å®ä¾‹
            vector_store_path: å‘é‡å­˜å‚¨è·¯å¾„
            chunk_size: æ–‡æ¡£åˆ†å—å¤§å°
            chunk_overlap: æ–‡æ¡£åˆ†å—é‡å å¤§å°
        """
        self.embeddings = embeddings_model
        self.llm = llm_model
        self.vector_store_path = vector_store_path
        self.vector_store = None
        self.retriever = None
        self.rag_chain = None

        # æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
        )

        # RAG æç¤ºæ¨¡æ¿
        self.prompt_template = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®åœ°è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦ç¼–é€ ç­”æ¡ˆã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚""",
                ),
                ("human", "{question}"),
            ]
        )

    def _setup_models(self):
        """è®¾ç½®æ¨¡å‹ï¼ˆå¦‚æœæœªæä¾›ï¼‰"""
        if self.embeddings is None:
            # å°è¯•ä½¿ç”¨ OpenAI
            try:
                from langchain_openai import OpenAIEmbeddings

                if not os.environ.get("OPENAI_API_KEY"):
                    os.environ["OPENAI_API_KEY"] = getpass.getpass(
                        "Enter OpenAI API key: "
                    )
                self.embeddings = OpenAIEmbeddings()
                print("âœ… ä½¿ç”¨ OpenAI Embeddings")
            except Exception as e:
                # å°è¯•ä½¿ç”¨æœ¬åœ°æ¨¡å‹æˆ–å…¶ä»–æœåŠ¡
                try:
                    from langchain_siliconflow import SiliconFlowEmbeddings

                    if not os.environ.get("SILICONFLOW_API_KEY"):
                        os.environ["SILICONFLOW_API_KEY"] = getpass.getpass(
                            "Enter SiliconFlow API key: "
                        )
                    self.embeddings = SiliconFlowEmbeddings()
                    print("âœ… ä½¿ç”¨ SiliconFlow Embeddings")
                except Exception:
                    print(f"âŒ æ— æ³•åˆå§‹åŒ–åµŒå…¥æ¨¡å‹: {e}")
                    print("è¯·å®‰è£… langchain-openai æˆ– langchain-siliconflow")
                    raise

        if self.llm is None:
            # å°è¯•ä½¿ç”¨ OpenAI
            try:
                from langchain_openai import ChatOpenAI

                if not os.environ.get("OPENAI_API_KEY"):
                    os.environ["OPENAI_API_KEY"] = getpass.getpass(
                        "Enter OpenAI API key: "
                    )
                self.llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
                print("âœ… ä½¿ç”¨ OpenAI Chat Model")
            except Exception as e:
                # å°è¯•ä½¿ç”¨å…¶ä»–æ¨¡å‹
                try:
                    from langchain.chat_models import init_chat_model

                    self.llm = init_chat_model("gpt-4o-mini")
                    print("âœ… ä½¿ç”¨ LangChain Chat Model")
                except Exception as ex:
                    print(f"âŒ æ— æ³•åˆå§‹åŒ– LLM æ¨¡å‹: {e}")
                    print(f"è¯¦ç»†é”™è¯¯: {ex}")
                    print("è¯·å®‰è£… langchain-openai æˆ–é…ç½®å…¶ä»–æ¨¡å‹")
                    raise

    def load_documents(
        self,
        source: str,
        file_type: Optional[str] = None,
    ) -> List[Document]:
        """
        åŠ è½½æ–‡æ¡£

        Args:
            source: æ–‡æ¡£è·¯å¾„ï¼ˆæ–‡ä»¶æˆ–ç›®å½•ï¼‰
            file_type: æ–‡ä»¶ç±»å‹ï¼ˆå¯é€‰ï¼Œè‡ªåŠ¨æ£€æµ‹ï¼‰

        Returns:
            æ–‡æ¡£åˆ—è¡¨
        """
        source_path = Path(source)

        if not source_path.exists():
            raise FileNotFoundError(f"è·¯å¾„ä¸å­˜åœ¨: {source}")

        documents = []

        # å¦‚æœæ˜¯ç›®å½•ï¼ŒåŠ è½½æ‰€æœ‰æ”¯æŒçš„æ–‡æ¡£
        if source_path.is_dir():
            # æ”¯æŒçš„æ–‡æ¡£ç±»å‹
            loaders = {
                "*.txt": TextLoader,
                "*.pdf": PyPDFLoader,
                "*.md": UnstructuredMarkdownLoader,
            }

            for pattern, loader_class in loaders.items():
                try:
                    loader = DirectoryLoader(
                        str(source_path),
                        glob=pattern,
                        loader_cls=loader_class,
                        show_progress=True,
                    )
                    docs = loader.load()
                    documents.extend(docs)
                    if docs:
                        print(f"âœ… åŠ è½½äº† {len(docs)} ä¸ª {pattern} æ–‡ä»¶")
                except Exception as e:
                    print(f"âš ï¸ åŠ è½½ {pattern} æ–‡ä»¶æ—¶å‡ºé”™: {e}")

        # å¦‚æœæ˜¯å•ä¸ªæ–‡ä»¶
        elif source_path.is_file():
            file_ext = source_path.suffix.lower()

            loader_map = {
                ".txt": TextLoader,
                ".pdf": PyPDFLoader,
                ".md": UnstructuredMarkdownLoader,
            }

            loader_class = loader_map.get(file_ext)
            if loader_class is None:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}")

            try:
                loader = loader_class(str(source_path))
                documents = loader.load()
                print(f"âœ… åŠ è½½äº†æ–‡æ¡£: {source_path.name}")
            except Exception as e:
                raise Exception(f"åŠ è½½æ–‡æ¡£å¤±è´¥: {e}")

        return documents

    def build_vector_store(
        self,
        documents: List[Document],
        force_rebuild: bool = False,
    ):
        """
        æ„å»ºå‘é‡å­˜å‚¨

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»º
        """
        if not documents:
            raise ValueError("æ–‡æ¡£åˆ—è¡¨ä¸ºç©º")

        self._setup_models()

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å‘é‡å­˜å‚¨
        vector_store_file = Path(self.vector_store_path) / "index.faiss"
        if vector_store_file.exists() and not force_rebuild:
            print(f"ğŸ“‚ åŠ è½½ç°æœ‰å‘é‡å­˜å‚¨: {self.vector_store_path}")
            try:
                self.vector_store = FAISS.load_local(
                    self.vector_store_path,
                    self.embeddings,
                    allow_dangerous_deserialization=True,
                )
                print("âœ… å‘é‡å­˜å‚¨åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½å¤±è´¥ï¼Œå°†é‡æ–°æ„å»º: {e}")
                force_rebuild = True

        if force_rebuild or self.vector_store is None:
            print("ğŸ”„ å¼€å§‹æ„å»ºå‘é‡å­˜å‚¨...")

            # åˆ†å‰²æ–‡æ¡£
            print(f"ğŸ“„ åˆ†å‰² {len(documents)} ä¸ªæ–‡æ¡£...")
            splits = self.text_splitter.split_documents(documents)
            print(f"âœ… åˆ†å‰²ä¸º {len(splits)} ä¸ªæ–‡æ¡£å—")

            # åˆ›å»ºå‘é‡å­˜å‚¨
            print("ğŸ”¢ ç”Ÿæˆå‘é‡åµŒå…¥...")
            self.vector_store = FAISS.from_documents(splits, self.embeddings)

            # ä¿å­˜å‘é‡å­˜å‚¨
            os.makedirs(self.vector_store_path, exist_ok=True)
            self.vector_store.save_local(self.vector_store_path)
            print(f"âœ… å‘é‡å­˜å‚¨å·²ä¿å­˜åˆ°: {self.vector_store_path}")

        # åˆ›å»ºæ£€ç´¢å™¨
        self.retriever = self.vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 4},  # æ£€ç´¢å‰ 4 ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£
        )

        # æ„å»º RAG é“¾
        self._build_rag_chain()

    def _build_rag_chain(self):
        """æ„å»º RAG é“¾"""

        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)

        self.rag_chain = (
            {
                "context": self.retriever | format_docs,
                "question": RunnablePassthrough(),
            }
            | self.prompt_template
            | self.llm
            | StrOutputParser()
        )
        print("âœ… RAG é“¾æ„å»ºå®Œæˆ")

    def query(self, question: str, verbose: bool = True) -> str:
        """
        æŸ¥è¯¢é—®é¢˜

        Args:
            question: ç”¨æˆ·é—®é¢˜
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

        Returns:
            å›ç­”
        """
        if self.rag_chain is None:
            raise ValueError("è¯·å…ˆæ„å»ºå‘é‡å­˜å‚¨ï¼ˆè°ƒç”¨ build_vector_storeï¼‰")

        if verbose:
            print(f"\nğŸ” æ£€ç´¢ç›¸å…³æ–‡æ¡£...")
            # å…ˆæ£€ç´¢ç›¸å…³æ–‡æ¡£
            relevant_docs = self.retriever.get_relevant_documents(question)
            print(f"ğŸ“š æ‰¾åˆ° {len(relevant_docs)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
            for i, doc in enumerate(relevant_docs[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ª
                preview = doc.page_content[:100].replace("\n", " ")
                print(f"   {i}. {preview}...")

        if verbose:
            print(f"ğŸ§  ç”Ÿæˆå›ç­”...")

        # ç”Ÿæˆå›ç­”
        answer = self.rag_chain.invoke(question)

        return answer

    def add_documents(self, documents: List[Document]):
        """
        æ·»åŠ æ–°æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨

        Args:
            documents: æ–°æ–‡æ¡£åˆ—è¡¨
        """
        if self.vector_store is None:
            raise ValueError("è¯·å…ˆæ„å»ºå‘é‡å­˜å‚¨")

        # åˆ†å‰²æ–°æ–‡æ¡£
        splits = self.text_splitter.split_documents(documents)

        # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
        self.vector_store.add_documents(splits)

        # ä¿å­˜æ›´æ–°åçš„å‘é‡å­˜å‚¨
        self.vector_store.save_local(self.vector_store_path)

        # é‡æ–°åˆ›å»ºæ£€ç´¢å™¨
        self.retriever = self.vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 4},
        )

        # é‡æ–°æ„å»º RAG é“¾
        self._build_rag_chain()

        print(f"âœ… å·²æ·»åŠ  {len(splits)} ä¸ªæ–‡æ¡£å—åˆ°å‘é‡å­˜å‚¨")


def main():
    """ä¸»å‡½æ•° - äº¤äº’å¼ RAG åº”ç”¨"""
    print("=" * 60)
    print("ğŸš€ RAG åº”ç”¨å¯åŠ¨")
    print("=" * 60)

    # åˆ›å»º RAG åº”ç”¨å®ä¾‹
    rag = RAGApplication(
        chunk_size=1000,
        chunk_overlap=200,
    )

    # æ£€æŸ¥æ˜¯å¦å·²æœ‰å‘é‡å­˜å‚¨
    vector_store_file = Path(rag.vector_store_path) / "index.faiss"

    if not vector_store_file.exists():
        print("\nğŸ“š é¦–æ¬¡ä½¿ç”¨ï¼Œéœ€è¦åŠ è½½æ–‡æ¡£æ„å»ºçŸ¥è¯†åº“")
        print("è¯·è¾“å…¥æ–‡æ¡£è·¯å¾„ï¼ˆæ–‡ä»¶æˆ–ç›®å½•ï¼‰ï¼š")
        doc_path = input("> ").strip()

        if not doc_path:
            # ä½¿ç”¨ç¤ºä¾‹æ–‡æ¡£
            print("âš ï¸ æœªæä¾›è·¯å¾„ï¼Œåˆ›å»ºç¤ºä¾‹æ–‡æ¡£...")
            example_doc = Document(
                page_content="""
                LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»º LLM åº”ç”¨çš„æ¡†æ¶ã€‚
                
                ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š
                1. æ–‡æ¡£åŠ è½½å’Œå¤„ç†
                2. å‘é‡å­˜å‚¨å’Œæ£€ç´¢
                3. é“¾å¼è°ƒç”¨å’Œç»„åˆ
                4. Agent å’Œå·¥å…·é›†æˆ
                
                RAG (Retrieval-Augmented Generation) æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„æŠ€æœ¯ã€‚
                å®ƒé€šè¿‡æ£€ç´¢ç›¸å…³æ–‡æ¡£æ¥å¢å¼º LLM çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½¿å›ç­”æ›´åŠ å‡†ç¡®å’Œå¯é ã€‚
                """,
                metadata={"source": "example.txt"},
            )
            documents = [example_doc]
        else:
            try:
                documents = rag.load_documents(doc_path)
            except Exception as e:
                print(f"âŒ åŠ è½½æ–‡æ¡£å¤±è´¥: {e}")
                return

        # æ„å»ºå‘é‡å­˜å‚¨
        try:
            rag.build_vector_store(documents, force_rebuild=True)
        except Exception as e:
            print(f"âŒ æ„å»ºå‘é‡å­˜å‚¨å¤±è´¥: {e}")
            return
    else:
        print(f"\nğŸ“‚ æ£€æµ‹åˆ°ç°æœ‰å‘é‡å­˜å‚¨: {rag.vector_store_path}")
        print("åŠ è½½å‘é‡å­˜å‚¨...")
        try:
            rag._setup_models()
            rag.vector_store = FAISS.load_local(
                rag.vector_store_path,
                rag.embeddings,
                allow_dangerous_deserialization=True,
            )
            rag.retriever = rag.vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 4},
            )
            rag._build_rag_chain()
            print("âœ… å‘é‡å­˜å‚¨åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ åŠ è½½å‘é‡å­˜å‚¨å¤±è´¥: {e}")
            return

    # äº¤äº’å¼é—®ç­”
    print("\n" + "=" * 60)
    print("ğŸ’¬ å¼€å§‹é—®ç­”ï¼ˆè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºï¼‰")
    print("=" * 60)

    while True:
        try:
            question = input("\nâ“ ä½ çš„é—®é¢˜: ").strip()

            if not question:
                continue

            if question.lower() in ["quit", "exit", "é€€å‡º", "q"]:
                print("\nğŸ‘‹ å†è§ï¼")
                break

            # æŸ¥è¯¢å¹¶æ˜¾ç¤ºå›ç­”
            answer = rag.query(question, verbose=True)
            print(f"\nğŸ¤– å›ç­”: {answer}")
            print("-" * 60)

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")


if __name__ == "__main__":
    main()
